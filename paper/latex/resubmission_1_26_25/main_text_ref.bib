@article{alonsoSentimentAnalysisFake2021,
  title = {Sentiment {{Analysis}} for {{Fake News Detection}}},
  author = {Alonso, Miguel A. and Vilares, David and {G{\'o}mez-Rodr{\'i}guez}, Carlos and Vilares, Jes{\'u}s},
  year = {2021},
  month = jan,
  journal = {Electronics},
  volume = {10},
  number = {11},
  pages = {1348},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics10111348},
  urldate = {2023-12-31},
  abstract = {In recent years, we have witnessed a rise in fake news, i.e., provably false pieces of information created with the intention of deception. The dissemination of this type of news poses a serious threat to cohesion and social well-being, since it fosters political polarization and the distrust of people with respect to their leaders. The huge amount of news that is disseminated through social media makes manual verification unfeasible, which has promoted the design and implementation of automatic systems for fake news detection. The creators of fake news use various stylistic tricks to promote the success of their creations, with one of them being to excite the sentiments of the recipients. This has led to sentiment analysis, the part of text analytics in charge of determining the polarity and strength of sentiments expressed in a text, to be used in fake news detection approaches, either as a basis of the system or as a complementary element. In this article, we study the different uses of sentiment analysis in the detection of fake news, with a discussion of the most relevant elements and shortcomings, and the requirements that should be met in the near future, such as multilingualism, explainability, mitigation of biases, or treatment of multimedia elements.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {fake news,opinion mining,sentiment analysis,social media},
  file = {C:\Users\griswold\Zotero\storage\A83LMEQP\Alonso et al. - 2021 - Sentiment Analysis for Fake News Detection.pdf}
}

@inproceedings{baccianellaSENTIWORDNETEnhancedLexical,
  title={Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining.},
  author={Baccianella, Stefano and Esuli, Andrea and Sebastiani, Fabrizio and others},
  booktitle={Lrec},
  volume={10},
  number={2010},
  pages={2200--2204},
  year={2010},
  organization={Valletta}
}

@article{beigiOverviewSentimentAnalysis2016,
  title = {An {{Overview}} of {{Sentiment Analysis}} in {{Social Media}} and {{Its Applications}} in {{Disaster Relief}}},
  author = {Beigi, Ghazaleh and Hu, Xia and Maciejewski, Ross and Liu, Huan},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  year = {2016},
  journal = {Sentiment Analysis and Ontology Engineering: An Environment of Computational Intelligence},
  series = {Studies in {{Computational Intelligence}}},
  pages = {313--340},
  doi = {10.1007/978-3-319-30319-2_13},
  urldate = {2023-12-31},
  abstract = {Sentiment analysis refers to the class of computational and natural language processingNatural language processingbasedSentiment analysistechniques used to identify, extract or characterize subjective information, such as opinions, expressed in a given piece of text. The main purpose of sentiment analysis is to classify a writer's attitude towards various topics into positive, negative or neutral categories. Sentiment analysis has many applications in different domains including, but not limited to, business intelligence, politics, sociology, etc. Recent years, on the other hand, have witnessed the advent of social networking websites, microblogs, wikis and Web applications and consequently, an unprecedented growth in user-generated data is poised for sentiment mining. Data such as web-postings, Tweets, videos, etc., all express opinions on various topics and events, offer immense opportunities to study and analyze human opinions and sentiment. In this chapter, we study the information published by individuals in social media in cases of natural disasters and emergencies and investigate if such information could be used by first responders to improve situational awareness and crisis management. In particular, we explore applications of sentiment analysisSentiment analysisand demonstrate how sentiment mining in social mediaSocial mediacan be exploited to determine how local crowds react during a disaster, and how such information can be used to improve disaster management. Such information can also be used to help assess the extent of the devastation and find people who are in specific need during an emergency situation. We first provide the formal definition of sentiment analysis in social media and cover traditional and the state-of-the-art approaches while highlighting contributions, shortcomings, and pitfalls due to the composition of online media streams. Next we discuss the relationship among social media, disaster reliefDisaster reliefand situational awareness and explain how social media is used in these contexts with the focus on sentiment analysis. In order to enable quick analysis of real-time geo-distributed data, we will detail applications of visual analytics with an emphasis on sentiment visualizationVisualization. Finally, we conclude the chapter with a discussion of research challenges in sentiment analysis and its application in disaster relief.},
  langid = {english},
  keywords = {Disaster relief,Sentiment analysis,Social media,Visualization}
}

@article{bestvaterSentimentNotStance2023,
  title = {Sentiment Is {{Not Stance}}: {{Target-Aware Opinion Classification}} for {{Political Text Analysis}}},
  shorttitle = {Sentiment Is {{Not Stance}}},
  author = {Bestvater, Samuel E. and Monroe, Burt L.},
  year = {2023},
  month = apr,
  journal = {Political Analysis},
  volume = {31},
  number = {2},
  pages = {235--256},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2022.10},
  urldate = {2023-09-05},
  abstract = {Sentiment analysis techniques have a long history in natural language processing and have become a standard tool in the analysis of political texts, promising a conceptually straightforward automated method of extracting meaning from textual data by scoring documents on a scale from positive to negative. However, while these kinds of sentiment scores can capture the overall tone of a document, the underlying concept of interest for political analysis is often actually the document's stance with respect to a given target---how positively or negatively it frames a specific idea, individual, or group---as this reflects the author's underlying political attitudes. In this paper, we question the validity of approximating author stance through sentiment scoring in the analysis of political texts, and advocate for greater attention to be paid to the conceptual distinction between a document's sentiment and its stance. Using examples from open-ended survey responses and from political discussions on social media, we demonstrate that in many political text analysis applications, sentiment and stance do not necessarily align, and therefore sentiment analysis methods fail to reliably capture ground-truth document stance, amplifying noise in the data and leading to faulty conclusions.},
  langid = {english},
  keywords = {machine learning,political stance,sentiment analysis,text-as-data},
  file = {C:\Users\griswold\Zotero\storage\EEFSY2GC\Bestvater and Monroe - 2023 - Sentiment is Not Stance Target-Aware Opinion Clas.pdf}
}

@article{boukesWhatToneEasy2020,
  title = {What's the {{Tone}}? {{Easy Doesn}}'t {{Do It}}: {{Analyzing Performance}} and {{Agreement Between Off-the-Shelf Sentiment Analysis Tools}}},
  shorttitle = {What's the {{Tone}}?},
  author = {Boukes, Mark and {van de Velde}, Bob and Araujo, Theo and Vliegenthart, Rens},
  year = {2020},
  month = apr,
  journal = {Communication Methods and Measures},
  volume = {14},
  number = {2},
  pages = {83--104},
  publisher = {Routledge},
  issn = {1931-2458},
  doi = {10.1080/19312458.2019.1671966},
  urldate = {2024-01-02},
  abstract = {This article scrutinizes the method of automated content analysis to measure the tone of news coverage. We compare a range of off-the-shelf sentiment analysis tools to manually coded economic news as well as examine the agreement between these dictionary approaches themselves. We assess the performance of five off-the-shelf sentiment analysis tools and two tailor-made dictionary-based approaches. The analyses result in five conclusions. First, there is little overlap between the off-the-shelf tools; causing wide divergence in terms of tone measurement. Second, there is no stronger overlap with manual coding for short texts (i.e., headlines) than for long texts (i.e., full articles). Third, an approach that combines individual dictionaries achieves a comparably good performance.~Fourth, precision may increase to acceptable levels at higher levels of granularity. Fifth, performance of dictionary approaches depends more on the number of relevant keywords in the dictionary than on the number of valenced words as such; a small tailor-made lexicon was not inferior to large established dictionaries. Altogether, we conclude that off-the-shelf sentiment analysis tools are mostly unreliable and unsuitable for research purposes -- at least in the context of Dutch economic news -- and manual validation for the specific language, domain, and genre of the research project at hand is always warranted.},
  file = {C:\Users\griswold\Zotero\storage\W7PLTDGR\Boukes et al. - 2020 - What’s the Tone Easy Doesn’t Do It Analyzing Per.pdf}
}

@article{cambriaSenticNetSemanticResource,
  title = {{{SenticNet}} 4: {{A Semantic Resource}} for {{Sentiment Analysis Based}} on {{Conceptual Primitives}}},
  author = {Cambria, Erik and Poria, Soujanya and Bajpai, Rajiv and Schuller, Bjoern},
  abstract = {An important difference between traditional AI systems and human intelligence is the human ability to harness commonsense knowledge gleaned from a lifetime of learning and experience to make informed decisions. This allows humans to adapt easily to novel situations where AI fails catastrophically due to a lack of situation-specific rules and generalization capabilities. Commonsense knowledge also provides background information that enables humans to successfully operate in social situations where such knowledge is typically assumed. Since commonsense consists of information that humans take for granted, gathering it is an extremely difficult task. Previous versions of SenticNet were focused on collecting this kind of knowledge for sentiment analysis but they were heavily limited by their inability to generalize. SenticNet 4 overcomes such limitations by leveraging on conceptual primitives automatically generated by means of hierarchical clustering and dimensionality reduction.},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\C87I5T9S\Cambria et al. - SenticNet 4 A Semantic Resource for Sentiment Ana.pdf}
}

@article{ceronEveryTweetCounts2014,
  title = {Every Tweet Counts? {{How}} Sentiment Analysis of Social Media Can Improve Our Knowledge of Citizens' Political Preferences with an Application to {{Italy}} and {{France}}},
  shorttitle = {Every Tweet Counts?},
  author = {Ceron, Andrea and Curini, Luigi and Iacus, Stefano M and Porro, Giuseppe},
  year = {2014},
  month = mar,
  journal = {New Media \& Society},
  volume = {16},
  number = {2},
  pages = {340--358},
  publisher = {SAGE Publications},
  issn = {1461-4448},
  doi = {10.1177/1461444813480466},
  urldate = {2024-01-02},
  abstract = {The growing usage of social media by a wider audience of citizens sharply increases the possibility of investigating the web as a device to explore and track political preferences. In the present paper we apply a method recently proposed by other social scientists to three different scenarios, by analyzing on one side the online popularity of Italian political leaders throughout 2011, and on the other the voting intention of French Internet users in both the 2012 presidential ballot and the subsequent legislative election. While Internet users are not necessarily representative of the whole population of a country's citizens, our analysis shows a remarkable ability for social media to forecast electoral results, as well as a noteworthy correlation between social media and the results of traditional mass surveys. We also illustrate that the predictive ability of social media analysis strengthens as the number of citizens expressing their opinion online increases, provided that the citizens act consistently on these opinions.},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\UXAZ2GAC\Ceron et al. - 2014 - Every tweet counts How sentiment analysis of soci.pdf}
}

@article{chauhanEmergenceSocialMedia2021,
  title = {The Emergence of Social Media Data and Sentiment Analysis in Election Prediction},
  author = {Chauhan, Priyavrat and Sharma, Nonita and Sikka, Geeta},
  year = {2021},
  month = feb,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  volume = {12},
  number = {2},
  pages = {2601--2627},
  issn = {1868-5145},
  doi = {10.1007/s12652-020-02423-y},
  urldate = {2023-09-05},
  abstract = {This work presents and assesses the power of various volumetric, sentiment, and social network approaches to predict crucial decisions from online social media platforms. The views of individuals play a vital role in the discovery of some critical decisions. Social media has become a well-known platform for voicing the feelings of the general population around the globe for almost decades. Sentiment analysis or opinion mining is a method that is used to mine the general population's views or feelings. In this respect, the forecasting of election results is an application of sentiment analysis aimed at predicting the outcomes of an ongoing election by gauging the mood of the public through social media. This survey paper outlines the evaluation of sentiment analysis techniques and tries to edify the contribution of the researchers to predict election results through social media content. This paper also gives a review of studies that tried to infer the political stance of online users using social media platforms such as Facebook and Twitter. Besides, this paper highlights the research challenges associated with predicting election results and open issues related to sentiment analysis. Further, this paper also suggests some future directions in respective election prediction using social media content.},
  langid = {english},
  keywords = {Election prediction,Opinion mining,Sentiment analysis,Social media,Twitter},
  file = {C:\Users\griswold\Zotero\storage\855M3BUV\Chauhan et al. - 2021 - The emergence of social media data and sentiment a.pdf}
}

@inproceedings{dengMPQAEntityEventLevel2015,
  title = {{{MPQA}} 3.0: {{An Entity}}/{{Event-Level Sentiment Corpus}}},
  shorttitle = {{{MPQA}} 3.0},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Deng, Lingjia and Wiebe, Janyce},
  editor = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
  year = {2015},
  month = may,
  pages = {1323--1328},
  publisher = {Association for Computational Linguistics},
  address = {Denver, Colorado},
  doi = {10.3115/v1/N15-1146},
  urldate = {2024-01-02},
  file = {C:\Users\griswold\Zotero\storage\7DCRXAGG\Deng and Wiebe - 2015 - MPQA 3.0 An EntityEvent-Level Sentiment Corpus.pdf}
}

@misc{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2024-01-12},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\MBL9EXJM\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;C\:\\Users\\griswold\\Zotero\\storage\\4S766QI5\\1810.html}
}

@article{drusSentimentAnalysisSocial2019,
  title = {Sentiment {{Analysis}} in {{Social Media}} and {{Its Application}}: {{Systematic Literature Review}}},
  shorttitle = {Sentiment {{Analysis}} in {{Social Media}} and {{Its Application}}},
  author = {Drus, Zulfadzli and Khalid, Haliyana},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The {{Fifth Information Systems International Conference}}, 23-24 {{July}} 2019, {{Surabaya}}, {{Indonesia}}},
  volume = {161},
  pages = {707--714},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.11.174},
  urldate = {2023-09-05},
  abstract = {This paper is a report of a review on sentiment analysis in social media that explored the methods, social media platform used and its application. Social media contain a large amount of raw data that has been uploaded by users in the form of text, videos, photos and audio. The data can be converted into valuable information by using sentiment analysis. A systematic review of studies published between 2014 to 2019 was undertaken using the following trusted and credible database including ACM, Emerald Insight, IEEE Xplore, Science Direct and Scopus. After the initial and in-depth screening of paper, 24 out of 77 articles have been chosen from the review process. The articles have been reviewed based on the aim of the study. The result shows most of the articles applied opinion-lexicon method to analyses text sentiment in social media, extracted data on microblogging site mainly Twitter and sentiment analysis application can be seen in world events, healthcare, politics and business.},
  keywords = {Big data,Sentiment analysis,Social media},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\PBBZMNJ3\\Drus and Khalid - 2019 - Sentiment Analysis in Social Media and Its Applica.pdf;C\:\\Users\\griswold\\Zotero\\storage\\CDKMZ2H7\\S187705091931885X.html}
}

@article{du2007stance,
  title = {The {{Stance Triangle}}},
  author = {Du Bois, John W},
  year = {2007},
  journal = {Stancetaking in discourse: Subjectivity, evaluation, interaction},
  volume = {164},
  number = {3},
  pages = {139--182}
}

@article{greavesUseSentimentAnalysis2013,
  title = {Use of {{Sentiment Analysis}} for {{Capturing Patient Experience From Free-Text Comments Posted Online}}},
  author = {Greaves, Felix and {Ramirez-Cano}, Daniel and Millett, Christopher and Darzi, Ara and Donaldson, Liam},
  year = {2013},
  month = nov,
  journal = {Journal of Medical Internet Research},
  volume = {15},
  number = {11},
  pages = {e2721},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/jmir.2721},
  urldate = {2023-12-31},
  abstract = {Background: There are large amounts of unstructured, free-text information about quality of health care available on the Internet in blogs, social networks, and on physician rating websites that are not captured in a systematic way. New analytical techniques, such as sentiment analysis, may allow us to understand and use this information more effectively to improve the quality of health care. Objective: We attempted to use machine learning to understand patients\&\#8217; unstructured comments about their care. We used sentiment analysis techniques to categorize online free-text comments by patients as either positive or negative descriptions of their health care. We tried to automatically predict whether a patient would recommend a hospital, whether the hospital was clean, and whether they were treated with dignity from their free-text description, compared to the patient\&\#8217;s own quantitative rating of their care. Methods: We applied machine learning techniques to all 6412 online comments about hospitals on the English National Health Service website in 2010 using Weka data-mining software. We also compared the results obtained from sentiment analysis with the paper-based national inpatient survey results at the hospital level using Spearman rank correlation for all 161 acute adult hospital trusts in England. Results: There was 81\%, 84\%, and 89\% agreement between quantitative ratings of care and those derived from free-text comments using sentiment analysis for cleanliness, being treated with dignity, and overall recommendation of hospital respectively (kappa scores: .40\&\#8211;.74, P\&\#60;.001 for all). We observed mild to moderate associations between our machine learning predictions and responses to the large patient survey for the three categories examined (Spearman rho 0.37-0.51, P\&\#60;.001 for all). Conclusions: The prediction accuracy that we have achieved using this machine learning process suggests that we are able to predict, from free-text, a reasonably accurate assessment of patients\&\#8217; opinion about different performance aspects of a hospital and that these machine learning predictions are associated with results of more conventional surveys.},
  langid = {english},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\2XFAH7QJ\\Greaves et al. - 2013 - Use of Sentiment Analysis for Capturing Patient Ex.pdf;C\:\\Users\\griswold\\Zotero\\storage\\VT7RKAQW\\e239.html}
}

@misc{hardalovSurveyStanceDetection2022a,
  title = {A {{Survey}} on {{Stance Detection}} for {{Mis-}} and {{Disinformation Identification}}},
  author = {Hardalov, Momchil and Arora, Arnav and Nakov, Preslav and Augenstein, Isabelle},
  year = {2022},
  month = may,
  number = {arXiv:2103.00242},
  eprint = {2103.00242},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00242},
  urldate = {2024-01-12},
  abstract = {Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\D97E9M8J\\Hardalov et al. - 2022 - A Survey on Stance Detection for Mis- and Disinfor.pdf;C\:\\Users\\griswold\\Zotero\\storage\\8SKZW4I8\\2103.html}
}

@article{hartmannMoreFeelingAccuracy2023,
  title = {More than a {{Feeling}}: {{Accuracy}} and {{Application}} of {{Sentiment Analysis}}},
  shorttitle = {More than a {{Feeling}}},
  author = {Hartmann, Jochen and Heitmann, Mark and Siebert, Christian and Schamp, Christina},
  year = {2023},
  month = mar,
  journal = {International Journal of Research in Marketing},
  volume = {40},
  number = {1},
  pages = {75--87},
  issn = {0167-8116},
  doi = {10.1016/j.ijresmar.2022.05.005},
  urldate = {2024-01-12},
  abstract = {Sentiment is fundamental to human communication. Countless marketing applications mine opinions from social media communication, news articles, customer feedback, or corporate communication. Various sentiment analysis methods are available and new ones have recently been proposed. Lexicons can relate individual words and expressions to sentiment scores. In contrast, machine learning methods are more complex to interpret, but promise higher accuracy, i.e., fewer false classifications. We propose an empirical framework and quantify these trade-offs for different types of research questions, data characteristics, and analytical resources to enable informed method decisions contingent on the application context. Based on a meta-analysis of 272 datasets and 12 million sentiment-labeled text documents, we find that the recently proposed transfer learning models indeed perform best, but can perform worse than popular leaderboard benchmarks suggest. We quantify the accuracy-interpretability trade-off, showing that, compared to widely established lexicons, transfer learning models on average classify more than 20 percentage points more documents correctly. To form realistic performance expectations, additional context variables, most importantly the desired number of sentiment classes and the text length, should be taken into account. We provide a pre-trained sentiment analysis model (called SiEBERT) with open-source scripts that can be applied as easily as an off-the-shelf lexicon.},
  keywords = {Deep Contextual Language Models,Machine Learning,Meta-Analysis,Natural Language Processing,Sentiment Analysis,Text Mining,Transfer Learning},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\T6CTZKIQ\\Hartmann et al. - 2023 - More than a Feeling Accuracy and Application of S.pdf;C\:\\Users\\griswold\\Zotero\\storage\\UUHWJHX6\\S0167811622000477.html}
}

@inproceedings{huMiningSummarizingCustomer2004a,
  title = {Mining and Summarizing Customer Reviews},
  booktitle = {Proceedings of the Tenth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Hu, Minqing and Liu, Bing},
  year = {2004},
  month = aug,
  pages = {168--177},
  publisher = {ACM},
  address = {Seattle WA USA},
  doi = {10.1145/1014052.1014073},
  urldate = {2024-01-02},
  isbn = {978-1-58113-888-7},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\X9F6LC54\Hu and Liu - 2004 - Mining and summarizing customer reviews.pdf}
}

@article{huttoVADERParsimoniousRuleBased2014,
  title = {{{VADER}}: {{A Parsimonious Rule-Based Model}} for {{Sentiment Analysis}} of {{Social Media Text}}},
  shorttitle = {{{VADER}}},
  author = {Hutto, C. and Gilbert, Eric},
  year = {2014},
  month = may,
  journal = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume = {8},
  number = {1},
  pages = {216--225},
  issn = {2334-0770},
  doi = {10.1609/icwsm.v8i1.14550},
  urldate = {2024-01-02},
  abstract = {The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a gold-standard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks.},
  copyright = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
  langid = {english},
  keywords = {Human Centered Computing},
  file = {C:\Users\griswold\Zotero\storage\DVBYYCM7\Hutto and Gilbert - 2014 - VADER A Parsimonious Rule-Based Model for Sentime.pdf}
}

@article{kazmaierPowerEnsembleLearning2022,
  title = {The Power of Ensemble Learning in Sentiment Analysis},
  author = {Kazmaier, Jacqueline and {van Vuuren}, Jan H.},
  year = {2022},
  month = jan,
  journal = {Expert Systems with Applications},
  volume = {187},
  pages = {115819},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2021.115819},
  urldate = {2023-09-05},
  abstract = {An ensemble of models is a set of learning models whose individual predictions are combined in such a way that component models compensate for each other's weaknesses. Although there has been a growing interest in ensemble learning techniques in the general machine learning community, the use of ensembles in sentiment classification is still limited. Moreover, much of the research activity on ensemble learning is centred around homogeneous ensembles, although heterogeneous ensembles may prove very useful when combining pre-trained models, which are often readily available. In this paper, several techniques for constructing heterogeneous ensembles are applied and comparatively evaluated in respect of benchmark sentiment classification data sets across four different domains. Median performance improvements of up to 5.53\% over the best individual model are observed for several ensemble configurations in respect of all four validation data sets, and clear trends are identified that may prove useful to other researchers in the field. Furthermore, a novel ensemble selection approach is proposed that avoids the storage of individual predictions, as well as the costly retraining of all candidate models for an ensemble, that are often required by other similar approaches.},
  keywords = {Ensemble learning,Machine learning,Natural language processing,Sentiment analysis},
  file = {C:\Users\griswold\Zotero\storage\WJU7YBF3\Kazmaier and van Vuuren - 2022 - The power of ensemble learning in sentiment analys.pdf}
}

@inproceedings{kumawatSentimentAnalysisUsing2021,
  title = {Sentiment {{Analysis Using Language Models}}: {{A Study}}},
  shorttitle = {Sentiment {{Analysis Using Language Models}}},
  booktitle = {2021 11th {{International Conference}} on {{Cloud Computing}}, {{Data Science}} \& {{Engineering}} ({{Confluence}})},
  author = {Kumawat, Spraha and Yadav, Inna and Pahal, Nisha and Goel, Deepti},
  year = {2021},
  month = jan,
  pages = {984--988},
  doi = {10.1109/Confluence51648.2021.9377043},
  urldate = {2024-05-13},
  abstract = {Sentiment analysis is concerned with extracting sentiment to ascertain the attitudes, and emotions associated with the text. It is broadly applied to voice of the customer as they convey their experience and feelings more blatantly which is why comprehending customer's emotions is a must. To identify and classify these unstructured emotions natural language processing (NLP) and machine learning approaches have been adopted in recent times. The main issue with the existing techniques is the inability to deal with the correct interpretation of context owing to lack of labeled data. In this paper, we studied deep neural network based language models to interpret and classify textual sequences into positive, negative or neutral emotions which remove the bottleneck of explicit human labeling. These models were analyzed and evaluations were performed on the Twitter US Airline Sentiment dataset. We have observed a considerable amount of improvements with respect to prior state-of-the-art approaches which closes the gap with supervised feature learning.},
  keywords = {Analytical models,Atmospheric modeling,Data models,Deep learning,Deep Learning,Language Models,Sentiment analysis,Sentiment Analysis,Social networking (online),Transformer cores,Transformers},
  file = {C:\Users\griswold\Zotero\storage\MBEDUR3G\9377043.html}
}

@article{baldi2000assessing,
  title={Assessing the accuracy of prediction algorithms for classification: an overview},
  author={Baldi, Pierre and Brunak, S{\o}ren and Chauvin, Yves and Andersen, Claus AF and Nielsen, Henrik},
  journal={Bioinformatics},
  volume={16},
  number={5},
  pages={412--424},
  year={2000},
  publisher={Oxford University Press}
}

@article{chicco2020advantages,
  title={The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
  author={Chicco, Davide and Jurman, Giuseppe},
  journal={BMC genomics},
  volume={21},
  pages={1--13},
  year={2020},
  publisher={Springer}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zhang2015cross,
  title={Cross-validation for selecting a model selection procedure},
  author={Zhang, Yongli and Yang, Yuhong},
  journal={Journal of Econometrics},
  volume={187},
  number={1},
  pages={95--112},
  year={2015},
  publisher={Elsevier}
}

@article{saleena2018ensemble,
  title={An ensemble classification system for twitter sentiment analysis},
  author={Saleena, Nabizath and others},
  journal={Procedia computer science},
  volume={132},
  pages={937--946},
  year={2018},
  publisher={Elsevier}
}

@article{chan2023state,
  title={State of the art: a review of sentiment analysis based on sequential transfer learning},
  author={Chan, Jireh Yi-Le and Bea, Khean Thye and Leow, Steven Mun Hong and Phoong, Seuk Wai and Cheng, Wai Khuen},
  journal={Artificial Intelligence Review},
  volume={56},
  number={1},
  pages={749--780},
  year={2023},
  publisher={Springer}
}

@article{barbieri2020tweeteval,
  title={Tweeteval: Unified benchmark and comparative evaluation for tweet classification},
  author={Barbieri, Francesco and Camacho-Collados, Jose and Neves, Leonardo and Espinosa-Anke, Luis},
  journal={arXiv preprint arXiv:2010.12421},
  year={2020}
}

@article{deng2022rlprompt,
  title={Rlprompt: Optimizing discrete text prompts with reinforcement learning},
  author={Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric P and Hu, Zhiting},
  journal={arXiv preprint arXiv:2205.12548},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{loureiro2022timelmsdiachroniclanguagemodels,
      title={TimeLMs: Diachronic Language Models from Twitter}, 
      author={Daniel Loureiro and Francesco Barbieri and Leonardo Neves and Luis Espinosa Anke and Jose Camacho-Collados},
      year={2022},
      eprint={2202.03829},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.03829}, 
}

@misc{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.11692},
  urldate = {2024-01-12},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\EEGKPKFA\\Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;C\:\\Users\\griswold\\Zotero\\storage\\RXHLPEKG\\1907.html}
}

@inproceedings{mohammadEmotionsEvokedCommon2010,
  title = {Emotions {{Evoked}} by {{Common Words}} and {{Phrases}}: {{Using Mechanical Turk}} to {{Create}} an {{Emotion Lexicon}}},
  shorttitle = {Emotions {{Evoked}} by {{Common Words}} and {{Phrases}}},
  booktitle = {Proceedings of the {{NAACL HLT}} 2010 {{Workshop}} on {{Computational Approaches}} to {{Analysis}} and {{Generation}} of {{Emotion}} in {{Text}}},
  author = {Mohammad, Saif and Turney, Peter},
  editor = {Inkpen, Diana and Strapparava, Carlo},
  year = {2010},
  month = jun,
  pages = {26--34},
  publisher = {Association for Computational Linguistics},
  address = {Los Angeles, CA},
  urldate = {2024-01-02},
  file = {C:\Users\griswold\Zotero\storage\CLSY2P73\Mohammad and Turney - 2010 - Emotions Evoked by Common Words and Phrases Using.pdf}
}

@inproceedings{mohammadSemEval2016TaskDetecting2016,
  title = {{{SemEval-2016 Task}} 6: {{Detecting Stance}} in {{Tweets}}},
  shorttitle = {{{SemEval-2016 Task}} 6},
  booktitle = {Proceedings of the 10th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval-2016}})},
  author = {Mohammad, Saif and Kiritchenko, Svetlana and Sobhani, Parinaz and Zhu, Xiaodan and Cherry, Colin},
  editor = {Bethard, Steven and Carpuat, Marine and Cer, Daniel and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
  year = {2016},
  month = jun,
  pages = {31--41},
  publisher = {Association for Computational Linguistics},
  address = {San Diego, California},
  doi = {10.18653/v1/S16-1003},
  urldate = {2023-12-31},
  file = {C:\Users\griswold\Zotero\storage\FT4WL5E9\Mohammad et al. - 2016 - SemEval-2016 Task 6 Detecting Stance in Tweets.pdf}
}

@article{alturayeif2023systematic,
  title={A systematic review of machine learning techniques for stance detection and its applications},
  author={Alturayeif, Nora and Luqman, Hamzah and Ahmed, Moataz},
  journal={Neural Computing and Applications},
  volume={35},
  number={7},
  pages={5113--5144},
  year={2023},
  publisher={Springer}
}

@inproceedings{sobhani2017dataset,
  title={A dataset for multi-target stance detection},
  author={Sobhani, Parinaz and Inkpen, Diana and Zhu, Xiaodan},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={551--557},
  year={2017}
}

@inproceedings{li2021multi,
  title={A multi-task learning framework for multi-target stance detection},
  author={Li, Yingjie and Caragea, Cornelia},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={2320--2326},
  year={2021}
}

@inproceedings{glandt2021stance,
  title={Stance detection in COVID-19 tweets},
  author={Glandt, Kyle and Khanal, Sarthak and Li, Yingjie and Caragea, Doina and Caragea, Cornelia},
  booktitle={Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (long papers)},
  volume={1},
  year={2021}
}

@article{sobhani2019exploring,
  title={Exploring deep neural networks for multitarget stance detection},
  author={Sobhani, Parinaz and Inkpen, Diana and Zhu, Xiaodan},
  journal={Computational Intelligence},
  volume={35},
  number={1},
  pages={82--97},
  year={2019},
  publisher={Wiley Online Library}
}

@article{mohammad2017stance,
  title={Stance and sentiment in tweets},
  author={Mohammad, Saif M and Sobhani, Parinaz and Kiritchenko, Svetlana},
  journal={ACM Transactions on Internet Technology (TOIT)},
  volume={17},
  number={3},
  pages={1--23},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{chicco2023matthews,
  title={The Matthews correlation coefficient (MCC) should replace the ROC AUC as the standard metric for assessing binary classification},
  author={Chicco, Davide and Jurman, Giuseppe},
  journal={BioData Mining},
  volume={16},
  number={1},
  pages={4},
  year={2023},
  publisher={Springer}
}

@inproceedings{nasukawaSentimentAnalysisCapturing2003,
  title = {Sentiment Analysis: Capturing Favorability Using Natural Language Processing},
  shorttitle = {Sentiment Analysis},
  booktitle = {Proceedings of the 2nd International Conference on {{Knowledge}} Capture},
  author = {Nasukawa, Tetsuya and Yi, Jeonghee},
  year = {2003},
  month = oct,
  series = {K-{{CAP}} '03},
  pages = {70--77},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/945645.945658},
  urldate = {2024-01-02},
  abstract = {This paper illustrates a sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from a document, instead of classifying the whole document into positive or negative.The essential issues in sentiment analysis are to identify how sentiments are expressed in texts and whether the expressions indicate positive (favorable) or negative (unfavorable) opinions toward the subject. In order to improve the accuracy of the sentiment analysis, it is important to properly identify the semantic relationships between the sentiment expressions and the subject. By applying semantic analysis with a syntactic parser and sentiment lexicon, our prototype system achieved high precision (75-95\%, depending on the data) in finding sentiments within Web pages and news articles.},
  isbn = {978-1-58113-583-1},
  keywords = {favorability analysis,information extraction,sentiment analysis,text mining},
  file = {C:\Users\griswold\Zotero\storage\I6PZKKAW\Nasukawa and Yi - 2003 - Sentiment analysis capturing favorability using n.pdf}
}

@misc{openaiIntroducingChatGPT,
  title = {Introducing {{ChatGPT}}},
  author = {OpenAI},
  urldate = {2024-01-12},
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  howpublished = {https://openai.com/blog/chatgpt},
  langid = {american},
  file = {C:\Users\griswold\Zotero\storage\3MMA4FHP\chatgpt.html},
  year = {2022}
}

@article{grimmer2013text,
  title={Text as data: The promise and pitfalls of automatic content analysis methods for political texts},
  author={Grimmer, Justin and Stewart, Brandon M},
  journal={Political analysis},
  volume={21},
  number={3},
  pages={267--297},
  year={2013},
  publisher={Cambridge University Press}
}

@misc{pangThumbsSentimentClassification2002,
  title = {Thumbs up? {{Sentiment Classification}} Using {{Machine Learning Techniques}}},
  shorttitle = {Thumbs Up?},
  author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  year = {2002},
  month = may,
  number = {arXiv:cs/0205070},
  eprint = {cs/0205070},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cs/0205070},
  urldate = {2024-01-02},
  abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.6,I.2.7},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\SDAB3ILM\\Pang et al. - 2002 - Thumbs up Sentiment Classification using Machine .pdf;C\:\\Users\\griswold\\Zotero\\storage\\IPX6UJXR\\0205070.html}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  year = {2018},
  file = {C:\Users\griswold\Zotero\storage\66AICYYZ\Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@inproceedings{socherRecursiveDeepModels2013,
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality Over}} a {{Sentiment Treebank}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
  editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
  year = {2013},
  month = oct,
  pages = {1631--1642},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, Washington, USA},
  urldate = {2024-01-12},
  file = {C:\Users\griswold\Zotero\storage\Y86CSNZV\Socher et al. - 2013 - Recursive Deep Models for Semantic Compositionalit.pdf}
}

@article{suAnalyzingPublicSentiments2017,
  title = {Analyzing Public Sentiments Online: Combining Human- and Computer-Based Content Analysis},
  shorttitle = {Analyzing Public Sentiments Online},
  author = {Su, Leona Yi-Fan and Cacciatore, Michael A. and Liang, Xuan and Brossard, Dominique and Scheufele, Dietram A. and Xenos, Michael A.},
  year = {2017},
  month = mar,
  journal = {Information, Communication \& Society},
  volume = {20},
  number = {3},
  pages = {406--427},
  publisher = {Routledge},
  issn = {1369-118X},
  doi = {10.1080/1369118X.2016.1182197},
  urldate = {2023-09-05},
  abstract = {Recent technological developments have created novel opportunities for analyzing and identifying patterns in large volumes of digital content. However, many content analysis tools require researchers to choose between the validity of human-based coding and the ability to analyze large volumes of content through computer-based techniques. This study argues for the use of supervised content analysis tools that capitalize on the strengths of human- and computer-based coding for assessing opinion expression. We begin by outlining the key methodological issues surrounding content analysis as performed by human coders and existing computational algorithms. After reviewing the most popular analytic approaches, we introduce an alternative, hybrid method that is aimed at improving reliability, validity, and efficiency when analyzing social media content. To demonstrate the usefulness of this method, we track nuclear energy- and nanotechnology-related opinion expression on Twitter surrounding the Fukushima Daiichi accident to examine the extent to which the volume and tone of tweets shift in directions consistent with the expected external influence of the event. Our analysis revealed substantial shifts in both the volume and tone of nuclear power-related tweets that were consistent with our expectations following the disaster event. Conversely, there was decidedly more stability in the volume and tone of tweets for our comparison issue. These analyses provide an empirical demonstration of how the presented hybrid method can analyze defined communication sentiment and topics from large-scale social media data sets. The implications for communication scholars are discussed.},
  keywords = {computer-based coding,Content analysis,human-based coding,sentiment analysis,supervised machine learning,Twitter},
  file = {C:\Users\griswold\Zotero\storage\E5ZQ75X5\Su et al. - 2017 - Analyzing public sentiments online combining huma.pdf}
}

@article{taboadaLexiconBasedMethodsSentiment2011,
  title = {Lexicon-{{Based Methods}} for {{Sentiment Analysis}}},
  author = {Taboada, Maite and Brooke, Julian and Tofiloski, Milan and Voll, Kimberly and Stede, Manfred},
  year = {2011},
  month = jun,
  journal = {Computational Linguistics},
  volume = {37},
  number = {2},
  pages = {267--307},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/COLI_a_00049},
  urldate = {2024-01-02},
  abstract = {We present a lexicon-based approach to extracting sentiment from text. The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensification and negation. SO-CAL is applied to the polarity classification task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and in completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\LAMZ77TX\Taboada et al. - 2011 - Lexicon-Based Methods for Sentiment Analysis.pdf}
}

@misc{wangLargeLanguageModels2023,
  title = {Large {{Language Models Are Zero-Shot Text Classifiers}}},
  author = {Wang, Zhiqiang and Pang, Yiran and Lin, Yanbin},
  year = {2023},
  month = dec,
  number = {arXiv:2312.01044},
  eprint = {2312.01044},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.01044},
  urldate = {2023-12-31},
  abstract = {Retrained large language models (LLMs) have become extensively used across various sub-disciplines of natural language processing (NLP). In NLP, text classification problems have garnered considerable focus, but still faced with some limitations related to expensive computational cost, time consumption, and robust performance to unseen classes. With the proposal of chain of thought prompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with the step by step reasoning prompts, instead of conventional question and answer formats. The zero-shot LLMs in the text classification problems can alleviate these limitations by directly utilizing pretrained models to predict both seen and unseen classes. Our research primarily validates the capability of GPT models in text classification. We focus on effectively utilizing prompt strategies to various text classification scenarios. Besides, we compare the performance of zero shot LLMs with other state of the art text classification methods, including traditional machine learning methods, deep learning methods, and ZSL methods. Experimental results demonstrate that the performance of LLMs underscores their effectiveness as zero-shot text classifiers in three of the four datasets analyzed. The proficiency is especially advantageous for small businesses or teams that may not have extensive knowledge in text classification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\C82IVZE3\\Wang et al. - 2023 - Large Language Models Are Zero-Shot Text Classifie.pdf;C\:\\Users\\griswold\\Zotero\\storage\\GSNAZZ28\\2312.html}
}

@article{wangSurveyZeroShotLearning2019,
  title = {A {{Survey}} of {{Zero-Shot Learning}}: {{Settings}}, {{Methods}}, and {{Applications}}},
  shorttitle = {A {{Survey}} of {{Zero-Shot Learning}}},
  author = {Wang, Wei and Zheng, Vincent W. and Yu, Han and Miao, Chunyan},
  year = {2019},
  month = jan,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {10},
  number = {2},
  pages = {13:1--13:37},
  issn = {2157-6904},
  doi = {10.1145/3293318},
  urldate = {2023-12-31},
  abstract = {Most machine-learning methods focus on classifying instances whose classes have already been seen in training. In practice, many applications require classifying instances whose classes have not been seen previously. Zero-shot learning is a powerful and promising learning paradigm, in which the classes covered by training instances and the classes we aim to classify are disjoint. In this paper, we provide a comprehensive survey of zero-shot learning. First of all, we provide an overview of zero-shot learning. According to the data utilized in model optimization, we classify zero-shot learning into three learning settings. Second, we describe different semantic spaces adopted in existing zero-shot learning works. Third, we categorize existing zero-shot learning methods and introduce representative methods under each category. Fourth, we discuss different applications of zero-shot learning. Finally, we highlight promising future research directions of zero-shot learning.},
  keywords = {Zero-shot learning survey},
  file = {C:\Users\griswold\Zotero\storage\RQ4AA89H\Wang et al. - 2019 - A Survey of Zero-Shot Learning Settings, Methods,.pdf}
}

@article{wankhadeSurveySentimentAnalysis2022,
  title = {A Survey on Sentiment Analysis Methods, Applications, and Challenges},
  author = {Wankhade, Mayur and Rao, Annavarapu Chandra Sekhara and Kulkarni, Chaitanya},
  year = {2022},
  month = oct,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {7},
  pages = {5731--5780},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10144-1},
  urldate = {2024-05-13},
  abstract = {The rapid growth of Internet-based applications, such as social media platforms and blogs, has resulted in comments and reviews concerning day-to-day activities. Sentiment analysis is the process of gathering and analyzing people's opinions, thoughts, and impressions regarding various topics, products, subjects, and services. People's opinions can be beneficial to corporations, governments, and individuals for collecting information and making decisions based on opinion. However, the sentiment analysis and evaluation procedure face numerous challenges. These challenges create impediments to accurately interpreting sentiments and determining the appropriate sentiment polarity. Sentiment analysis identifies and extracts subjective information from the text using natural language processing and text mining. This article discusses a complete overview of the method for completing this task as well as the applications of sentiment analysis. Then, it evaluates, compares, and investigates the approaches used to gain a comprehensive understanding of their advantages and disadvantages. Finally, the challenges of sentiment analysis are examined in order to define future directions.},
  langid = {english},
  keywords = {Machine learning,Sentiment analysis,Social media,Text analysis,Word embedding},
  file = {C:\Users\griswold\Zotero\storage\NRFJ38NZ\Wankhade et al. - 2022 - A survey on sentiment analysis methods, applicatio.pdf}
}

@misc{stewartLabelFreeSupervisionNeural2016,
  title = {Label-{{Free Supervision}} of {{Neural Networks}} with {{Physics}} and {{Domain Knowledge}}},
  author = {Stewart, Russell and Ermon, Stefano},
  year = {2016},
  month = sep,
  number = {arXiv:1609.05566},
  eprint = {1609.05566},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-11},
  abstract = {In many machine learning applications, labeled data is scarce and obtaining more labels is expensive. We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs. These constraints are derived from prior domain knowledge, e.g., from known laws of physics. We demonstrate the effectiveness of this approach on real world and simulated computer vision tasks. We are able to train a convolutional neural network to detect and track objects without any labeled examples. Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C:\Users\griswold\Zotero\storage\VYKS5C8C\Stewart and Ermon - 2016 - Label-Free Supervision of Neural Networks with Phy.pdf}
}


@article{roeFeatureEngineeringClinical2020,
  title = {Feature Engineering with Clinical Expert Knowledge: {{A}} Case Study Assessment of Machine Learning Model Complexity and Performance},
  shorttitle = {Feature Engineering with Clinical Expert Knowledge},
  author = {Roe, Kenneth D. and Jawa, Vibhu and Zhang, Xiaohan and Chute, Christopher G. and Epstein, Jeremy A. and Matelsky, Jordan and Shpitser, Ilya and Taylor, Casey Overby},
  year = {2020},
  month = apr,
  journal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0231300},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0231300},
  urldate = {2024-06-11},
  abstract = {Incorporating expert knowledge at the time machine learning models are trained holds promise for producing models that are easier to interpret. The main objectives of this study were to use a feature engineering approach to incorporate clinical expert knowledge prior to applying machine learning techniques, and to assess the impact of the approach on model complexity and performance. Four machine learning models were trained to predict mortality with a severe asthma case study. Experiments to select fewer input features based on a discriminative score showed low to moderate precision for discovering clinically meaningful triplets, indicating that discriminative score alone cannot replace clinical input. When compared to baseline machine learning models, we found a decrease in model complexity with use of fewer features informed by discriminative score and filtering of laboratory features with clinical input. We also found a small difference in performance for the mortality prediction task when comparing baseline ML models to models that used filtered features. Encoding demographic and triplet information in ML models with filtered features appeared to show performance improvements from the baseline. These findings indicated that the use of filtered features may reduce model complexity, and with little impact on performance.},
  langid = {english},
  keywords = {Asthma,Clinical laboratories,Electronic medical records,Laboratory tests,Machine learning,Machine learning algorithms,Medical risk factors,Neural networks},
  file = {C:\Users\griswold\Zotero\storage\IZLRYN7K\Roe et al. - 2020 - Feature engineering with clinical expert knowledge.pdf}
}


@article{vonruedenInformedMachineLearning2021,
  title = {Informed {{Machine Learning}} - {{A Taxonomy}} and {{Survey}} of {{Integrating Prior Knowledge}} into {{Learning Systems}}},
  author = {Von Rueden, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Walczak, Michal and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis},
  year = {2021},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2021.3079836},
  urldate = {2024-06-11},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  file = {C:\Users\griswold\Zotero\storage\RIKQEB4V\Von Rueden et al. - 2021 - Informed Machine Learning - A Taxonomy and Survey .pdf}
}


@article{widmannCreatingComparingDictionary2022,
  title = {Creating and {{Comparing Dictionary}}, {{Word Embedding}}, and {{Transformer-Based Models}} to {{Measure Discrete Emotions}} in {{German Political Text}}},
  author = {Widmann, Tobias and Wich, Maximilian},
  year = {2022},
  month = jun,
  journal = {Political Analysis},
  pages = {1--16},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2022.15},
  urldate = {2023-09-05},
  abstract = {Previous research on emotional language relied heavily on off-the-shelf sentiment dictionaries that focus on negative and positive tone. These dictionaries are often tailored to nonpolitical domains and use bag-of-words approaches which come with a series of disadvantages. This paper creates, validates, and compares the performance of (1) a novel emotional dictionary specifically for political text, (2) locally trained word embedding models combined with simple neural network classifiers, and (3) transformer-based models which overcome limitations of the dictionary approach. All tools can measure emotional appeals associated with eight discrete emotions. The different approaches are validated on different sets of crowd-coded sentences. Encouragingly, the results highlight the strengths of novel transformer-based models, which come with easily available pretrained language models. Furthermore, all customized approaches outperform widely used off-the-shelf dictionaries in measuring emotional language in German political discourse.},
  langid = {english},
  keywords = {dictionary,emotions,political text,text-as-data,transformer models,word embeddings},
  file = {C:\Users\griswold\Zotero\storage\TW2DRWDW\Widmann and Wich - 2022 - Creating and Comparing Dictionary, Word Embedding,.pdf}
}

@inproceedings{diligentiIntegratingPriorKnowledge2017,
  title = {Integrating {{Prior Knowledge}} into {{Deep Learning}}},
  booktitle = {2017 16th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Diligenti, Michelangelo and Roychowdhury, Soumali and Gori, Marco},
  year = {2017},
  month = dec,
  pages = {920--923},
  publisher = {IEEE},
  address = {Cancun, Mexico},
  doi = {10.1109/ICMLA.2017.00-37},
  urldate = {2024-06-11},
  isbn = {978-1-5386-1418-1}
}


@article{yarchiPoliticalPolarizationDigital2021,
  title = {Political {{Polarization}} on the {{Digital Sphere}}: {{A Cross-platform}}, {{Over-time Analysis}} of {{Interactional}}, {{Positional}}, and {{Affective Polarization}} on {{Social Media}}},
  shorttitle = {Political {{Polarization}} on the {{Digital Sphere}}},
  author = {Yarchi, Moran and Baden, Christian and {Kligler-Vilenchik}, Neta},
  year = {2021},
  month = mar,
  journal = {Political Communication},
  volume = {38},
  number = {1-2},
  pages = {98--139},
  publisher = {Routledge},
  issn = {1058-4609},
  doi = {10.1080/10584609.2020.1785067},
  urldate = {2024-01-02},
  abstract = {Political polarization on the digital sphere poses a real challenge to many democracies around the world. Although the issue has received some scholarly attention, there is a need to improve the conceptual precision in the increasingly blurry debate. The use of computational communication science approaches allows us to track political conversations in a fine-grained manner within their natural settings -- the realm of interactive social media. The present study combines different algorithmic approaches to studying social media data in order to capture both the interactional structure and content of dynamic political talk online. We conducted an analysis of political polarization across social media platforms (analyzing Facebook, Twitter, and WhatsApp) over 16~months, with close to a quarter million online contributions regarding a political controversy in Israel. Our comprehensive measurement of interactive political talk enables us to address three key aspects of political polarization: (1) interactional polarization -- homophilic versus heterophilic user interactions; (2) positional polarization -- the positions expressed, and (3) affective polarization -- the emotions and attitudes expressed. Our findings indicate that political polarization on social media cannot be conceptualized as a unified phenomenon, as there are significant cross-platform differences. While interactions on Twitter largely conform to established expectations (homophilic interaction patterns, aggravating positional polarization, pronounced inter-group hostility), on WhatsApp, de-polarization occurred over time. Surprisingly, Facebook was found to be the least homophilic platform in terms of interactions, positions, and emotions expressed. Our analysis points to key conceptual distinctions and raises important questions about the drivers and dynamics of political polarization online.},
  keywords = {computational communication science approach,cross-platform analysis,over-time analysis,Political polarization,social media},
  file = {C:\Users\griswold\Zotero\storage\77PRYUNZ\Yarchi et al. - 2021 - Political Polarization on the Digital Sphere A Cr.pdf}
}

@misc{zhangSentimentAnalysisEra2023,
  title = {Sentiment {{Analysis}} in the {{Era}} of {{Large Language Models}}: {{A Reality Check}}},
  shorttitle = {Sentiment {{Analysis}} in the {{Era}} of {{Large Language Models}}},
  author = {Zhang, Wenxuan and Deng, Yue and Liu, Bing and Pan, Sinno Jialin and Bing, Lidong},
  year = {2023},
  month = may,
  number = {arXiv:2305.15005},
  eprint = {2305.15005},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-13},
  abstract = {Sentiment analysis (SA) has been a longstanding research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, SENTIEVAL, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at https://github. com/DAMO-NLP-SG/LLM-Sentiment.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\griswold\Zotero\storage\U4AE3SWC\Zhang et al. - 2023 - Sentiment Analysis in the Era of Large Language Mo.pdf}
}

@misc{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-11},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\griswold\Zotero\storage\82PML6JR\Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf}
}

@misc{zhongCanChatGPTUnderstand2023,
  title = {Can {{ChatGPT Understand Too}}? {{A Comparative Study}} on {{ChatGPT}} and {{Fine-tuned BERT}}},
  shorttitle = {Can {{ChatGPT Understand Too}}?},
  author = {Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  year = {2023},
  month = mar,
  number = {arXiv:2302.10198},
  eprint = {2302.10198},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-11},
  abstract = {Recently, ChatGPT has attracted great attention, as it can generate fluent and high-quality responses to human inquiries. Several prior studies have shown that ChatGPT attains remarkable generation ability compared with existing models. However, the quantitative analysis of ChatGPT's understanding ability has been given little attention. In this report, we explore the understanding ability of ChatGPT by evaluating it on the most popular GLUE benchmark, and comparing it with 4 representative fine-tuned BERT-style models. We find that: 1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT outperforms all BERT models on inference tasks by a large margin; 3) ChatGPT achieves comparable performance compared with BERT on sentiment analysis and question-answering tasks. Additionally, by combining some advanced prompting strategies, we show that the understanding ability of ChatGPT can be further improved.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\griswold\Zotero\storage\QIJT3P8D\Zhong et al. - 2023 - Can ChatGPT Understand Too A Comparative Study on.pdf}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{pollard24,
  title={A Demonstration of Propensity Score Weighting to Successfully Adjust a Social Media Convenience Sample Survey of Political Attitudes},
  author={Pollard, Michael S. and Robbins, Michael W. and Griswold, Max G.},
  journal={Preprint},
  year={2024}
}

@online{aiyappaCanWeTrust2024,
  title = {Can We Trust the Evaluation on {{ChatGPT}}?},
  author = {Aiyappa, Rachith and An, Jisun and Kwak, Haewoon and Ahn, Yong-Yeol},
  date = {2024-08-22},
  eprint = {2303.12767},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.12767},
  url = {http://arxiv.org/abs/2303.12767},
  urldate = {2024-11-20},
  abstract = {ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\4I8PNH29\\Aiyappa et al. - 2024 - Can we trust the evaluation on ChatGPT.pdf;C\:\\Users\\griswold\\Zotero\\storage\\S8GGGP3W\\2303.html}
}

@online{alizadehOpenSourceLLMsText2024,
  title = {Open-{{Source LLMs}} for {{Text Annotation}}: {{A Practical Guide}} for {{Model Setting}} and {{Fine-Tuning}}},
  shorttitle = {Open-{{Source LLMs}} for {{Text Annotation}}},
  author = {Alizadeh, Meysam and Kubli, Maël and Samei, Zeynab and Dehghani, Shirin and Zahedivafa, Mohammadmasiha and Bermeo, Juan Diego and Korobeynikova, Maria and Gilardi, Fabrizio},
  date = {2024-05-29},
  eprint = {2307.02179},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2307.02179},
  url = {http://arxiv.org/abs/2307.02179},
  urldate = {2024-11-21},
  abstract = {This paper studies the performance of open-source Large Language Models (LLMs) in text classification tasks typical for political science research. By examining tasks like stance, topic, and relevance classification, we aim to guide scholars in making informed decisions about their use of LLMs for text analysis. Specifically, we conduct an assessment of both zero-shot and fine-tuned LLMs across a range of text annotation tasks using news articles and tweets datasets. Our analysis shows that fine-tuning improves the performance of open-source LLMs, allowing them to match or even surpass zero-shot GPT-3.5 and GPT-4, though still lagging behind fine-tuned GPT-3.5. We further establish that fine-tuning is preferable to few-shot training with a relatively modest quantity of annotated text. Our findings show that fine-tuned open-source LLMs can be effectively deployed in a broad spectrum of text annotation applications. We provide a Python notebook facilitating the application of LLMs in text annotation for other researchers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\PJ8PQUTX\\Alizadeh et al. - 2024 - Open-Source LLMs for Text Annotation A Practical .pdf;C\:\\Users\\griswold\\Zotero\\storage\\JKTUC5XZ\\2307.html}
}

@article{aroyoTruthLieCrowd2015,
  title = {Truth {{Is}} a {{Lie}}: {{Crowd Truth}} and the {{Seven Myths}} of {{Human Annotation}}},
  shorttitle = {Truth {{Is}} a {{Lie}}},
  author = {Aroyo, Lora and Welty, Chris},
  date = {2015-03-25},
  journaltitle = {AI Magazine},
  volume = {36},
  number = {1},
  pages = {15--24},
  issn = {2371-9621},
  doi = {10.1609/aimag.v36i1.2564},
  url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2564},
  urldate = {2024-11-20},
  abstract = {Big data is having a disruptive impact across the sciences. Human annotation of semantic interpretation tasks is a critical part of big data semantics, but it is based on an antiquated ideal of a single correct truth that needs to be similarly disrupted. We expose seven myths about human annotation, most of which derive from that antiquated ideal of truth, and dispell these myths with examples from our research. We propose a new theory of truth, crowd truth, that is based on the intuition that human interpretation is subjective, and that measuring annotations on the same objects of interpretation (in our examples, sentences) across a crowd will provide a useful representation of their subjectivity and the range of reasonable interpretations.},
  issue = {1},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\VBML9JC3\Aroyo and Welty - 2015 - Truth Is a Lie Crowd Truth and the Seven Myths of.pdf}
}

@inproceedings{brownLanguageModelsAre2020a,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate = {2024-11-20},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {C:\Users\griswold\Zotero\storage\NZPST5V7\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@online{burnhamStanceDetectionPractical2024,
  title = {Stance {{Detection}}: {{A Practical Guide}} to {{Classifying Political Beliefs}} in {{Text}}},
  shorttitle = {Stance {{Detection}}},
  author = {Burnham, Michael},
  date = {2024-05-03},
  eprint = {2305.01723},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2305.01723},
  url = {http://arxiv.org/abs/2305.01723},
  urldate = {2024-11-21},
  abstract = {Stance detection is identifying expressed beliefs in a document. While researchers widely use sentiment analysis for this, recent research demonstrates that sentiment and stance are distinct. This paper advances text analysis methods by precisely defining stance detection and presenting three distinct approaches: supervised classification, natural language inference, and in-context learning with generative language models. I discuss how document context and trade-offs between resources and workload should inform your methods. For all three approaches I provide guidance on application and validation techniques, as well as coding tutorials for implementation. Finally, I demonstrate how newer classification approaches can replicate supervised classifiers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\HZ5LE2UA\\Burnham - 2024 - Stance Detection A Practical Guide to Classifying.pdf;C\:\\Users\\griswold\\Zotero\\storage\\VDY8QKMG\\2305.html}
}

@online{caoLanguageModelsLearn2024,
  title = {Language {{Models Learn Metadata}}: {{Political Stance Detection Case Study}}},
  shorttitle = {Language {{Models Learn Metadata}}},
  author = {Cao, Stanley and Drinkall, Felix},
  date = {2024-09-15},
  eprint = {2409.13756},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2409.13756},
  urldate = {2024-10-23},
  abstract = {Stance detection is a crucial NLP task with numerous applications in social science, from analyzing online discussions to assessing political campaigns. This paper investigates the optimal way to incorporate metadata into a political stance detection task. We demonstrate that previous methods combining metadata with language-based data for political stance detection have not fully utilized the metadata information; our simple baseline, using only party membership information, surpasses the current state-of-the-art. We then show that prepending metadata (e.g., party and policy) to political speeches performs best, outperforming all baselines, indicating that complex metadata inclusion systems may not learn the task optimally.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\94MKPHJK\\Cao and Drinkall - 2024 - Language Models Learn Metadata Political Stance D.pdf;C\:\\Users\\griswold\\Zotero\\storage\\IG96B6LL\\2409.html}
}

@article{chiccoAdvantagesMatthewsCorrelation2020,
  title = {The Advantages of the {{Matthews}} Correlation Coefficient ({{MCC}}) over {{F1}} Score and Accuracy in Binary Classification Evaluation},
  author = {Chicco, Davide and Jurman, Giuseppe},
  date = {2020-01-02},
  journaltitle = {BMC Genomics},
  shortjournal = {BMC Genomics},
  volume = {21},
  number = {1},
  pages = {6},
  issn = {1471-2164},
  doi = {10.1186/s12864-019-6413-7},
  url = {https://doi.org/10.1186/s12864-019-6413-7},
  urldate = {2024-12-20},
  abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.},
  keywords = {Accuracy,Binary classification,Biostatistics,Confusion matrices,Dataset imbalance,F1 score,Genomics,Machine learning,Matthews correlation coefficient},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\M64PHYP4\\Chicco and Jurman - 2020 - The advantages of the Matthews correlation coeffic.pdf;C\:\\Users\\griswold\\Zotero\\storage\\VWEEI3FY\\s12864-019-6413-7.html}
}

@article{chiccoMatthewsCorrelationCoefficient2023,
  title = {The {{Matthews}} Correlation Coefficient~({{MCC}}) Should Replace the {{ROC~AUC}} as the Standard Metric for Assessing Binary Classification},
  author = {Chicco, Davide and Jurman, Giuseppe},
  date = {2023-02-17},
  journaltitle = {BioData Mining},
  shortjournal = {BioData Mining},
  volume = {16},
  number = {1},
  pages = {4},
  issn = {1756-0381},
  doi = {10.1186/s13040-023-00322-4},
  url = {https://doi.org/10.1186/s13040-023-00322-4},
  urldate = {2024-11-21},
  abstract = {Binary classification is a common task for which machine learning and computational statistics are used, and the area under the receiver operating characteristic curve~(ROC~AUC) has become the common standard metric to evaluate binary classifications in most scientific fields. The ROC curve has true positive rate~(also called sensitivity or recall) on the y axis and false positive rate on the x axis, and the ROC~AUC can range from 0 (worst result) to 1~(perfect result). The ROC~AUC, however, has several flaws and drawbacks. This score is generated including predictions that obtained insufficient sensitivity and specificity, and moreover it does not say anything about positive predictive value~(also known as precision) nor negative predictive value~(NPV) obtained by the classifier, therefore potentially generating inflated overoptimistic results. Since it is common to include ROC~AUC alone without precision and negative predictive value, a researcher might erroneously conclude that their classification was successful. Furthermore, a given point in the ROC space does not identify a single confusion matrix nor a group of matrices sharing the same MCC value. Indeed, a given (sensitivity, specificity) pair can cover a broad MCC range, which casts doubts on the reliability of ROC~AUC as a performance measure. In contrast, the Matthews correlation coefficient~(MCC) generates a high score in its \$\$[-1; +1]\$\$interval only if the classifier scored a high value for all the four basic rates of the confusion matrix: sensitivity, specificity, precision, and negative predictive value. A high MCC (for example, MCC \$\$=\$\$0.9), moreover, always corresponds to a high ROC~AUC, and not vice versa. In this short study, we explain why the Matthews correlation coefficient should replace the ROC~AUC as standard statistic in all the scientific studies involving a binary classification, in all scientific fields.},
  keywords = {Area under the curve,AUC,Binary classification,Confusion matrix,Data mining,Data science,Matthews correlation coefficient,Receiver operating characteristic curve,ROC,ROC AUC,Supervised machine learning},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\WKPJTUWG\\Chicco and Jurman - 2023 - The Matthews correlation coefficient (MCC) should .pdf;C\:\\Users\\griswold\\Zotero\\storage\\77TKEK6L\\s13040-023-00322-4.html}
}

@article{rozado2024political,
  title={The political preferences of llms},
  author={Rozado, David},
  journal={arXiv preprint arXiv:2402.01789},
  year={2024}
}

@book{gelman1995bayesian,
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
  year={1995},
  publisher={Chapman and Hall/CRC}
}

@book{bartholomew2011latent,
  title={Latent variable models and factor analysis: A unified approach},
  author={Bartholomew, David J and Knott, Martin and Moustaki, Irini},
  year={2011},
  publisher={John Wiley \& Sons}
}

@article{royston2006dichotomizing,
  title={Dichotomizing continuous predictors in multiple regression: a bad idea},
  author={Royston, Patrick and Altman, Douglas G and Sauerbrei, Willi},
  journal={Statistics in medicine},
  volume={25},
  number={1},
  pages={127--141},
  year={2006},
  publisher={Wiley Online Library}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, T},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{van2008leave,
  title={Leave ‘em alone--why continuous variables should be analyzed as such},
  author={Van Walraven, Carl and Hart, Robert G},
  journal={Neuroepidemiology},
  volume={30},
  number={3},
  pages={138--139},
  year={2008},
  publisher={S. Karger AG Basel, Switzerland}
}

@article{gelman2009splitting,
  title={Splitting a predictor at the upper quarter or third and the lower quarter or third},
  author={Gelman, Andrew and Park, David K},
  journal={The American Statistician},
  volume={63},
  number={1},
  pages={1--8},
  year={2009},
  publisher={Taylor \& Francis}
}

@online{chuangTutorialsStanceDetection2023,
  title = {Tutorials on {{Stance Detection}} Using {{Pre-trained Language Models}}: {{Fine-tuning BERT}} and {{Prompting Large Language Models}}},
  shorttitle = {Tutorials on {{Stance Detection}} Using {{Pre-trained Language Models}}},
  author = {Chuang, Yun-Shiuan},
  date = {2023-07-28},
  eprint = {2307.15331},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2307.15331},
  url = {http://arxiv.org/abs/2307.15331},
  urldate = {2024-11-22},
  abstract = {This paper presents two self-contained tutorials on stance detection in Twitter data using BERT fine-tuning and prompting large language models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models with HuggingFace transformers. The second focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning. Various prompting strategies are implemented and evaluated using confusion matrices and macro F1 scores. The tutorials provide code, visualizations, and insights revealing the strengths of few-shot ChatGPT and FLAN-T5 which outperform fine-tuned BERTs. By covering both model fine-tuning and prompting-based techniques in an accessible, hands-on manner, these tutorials enable learners to gain applied experience with cutting-edge methods for stance detection.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\KU7D3QMH\\Chuang - 2023 - Tutorials on Stance Detection using Pre-trained La.pdf;C\:\\Users\\griswold\\Zotero\\storage\\8E5V6IFD\\2307.html}
}

@online{cruickshankPromptingFineTuningOpenSourced2024,
  title = {Prompting and {{Fine-Tuning Open-Sourced Large Language Models}} for {{Stance Classification}}},
  author = {Cruickshank, Iain J. and Ng, Lynnette Hui Xian},
  date = {2024-03-05},
  eprint = {2309.13734},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2309.13734},
  url = {http://arxiv.org/abs/2309.13734},
  urldate = {2024-11-22},
  abstract = {Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for manual annotations. We investigate 10 open-source models and 7 prompting schemes, finding that LLMs are competitive with in-domain supervised models but are not necessarily consistent in their performance. We also fine-tuned the LLMs, but discovered that fine-tuning process does not necessarily lead to better performance. In general, we discover that LLMs do not routinely outperform their smaller supervised machine learning models, and thus call for stance detection to be a benchmark for which LLMs also optimize for. The code used in this study is available at \textbackslash url\{https://github.com/ijcruic/LLM-Stance-Labeling\}},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\6KBZB9ZJ\\Cruickshank and Ng - 2024 - Prompting and Fine-Tuning Open-Sourced Large Langu.pdf;C\:\\Users\\griswold\\Zotero\\storage\\VSX3PQVX\\2309.html}
}

@inproceedings{dingCrossTargetStanceDetection2024,
  title = {Cross-{{Target Stance Detection}} by {{Exploiting Target Analytical Perspectives}}},
  booktitle = {{{ICASSP}} 2024 - 2024 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ding, Daijun and Chen, Rong and Jing, Liwen and Zhang, Bowen and Huang, Xu and Dong, Li and Zhao, Xiaowen and Song, Ge},
  date = {2024-04},
  pages = {10651--10655},
  issn = {2379-190X},
  doi = {10.1109/ICASSP48485.2024.10448397},
  url = {https://ieeexplore.ieee.org/document/10448397},
  urldate = {2024-11-21},
  abstract = {Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target. One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets. However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge. In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge. First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM). Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments results demonstrate the superiority of MPPT against the state-of-the-art baseline methods.},
  eventtitle = {{{ICASSP}} 2024 - 2024 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {Acoustics,Analytical models,Bridges,chain-of-thought,cross-target stance detection,Feature extraction,Fuses,prompt-tuning,Signal processing,stance detection,Task analysis},
  file = {C:\Users\griswold\Zotero\storage\RSXSWHA2\Ding et al. - 2024 - Cross-Target Stance Detection by Exploiting Target.pdf}
}

@article{dingLeveragingChainofThoughtEnhance2024,
  title = {Leveraging {{Chain-of-Thought}} to {{Enhance Stance Detection}} with {{Prompt-Tuning}}},
  author = {Ding, Daijun and Fu, Xianghua and Peng, Xiaojiang and Fan, Xiaomao and Huang, Hu and Zhang, Bowen},
  date = {2024-01},
  journaltitle = {Mathematics},
  volume = {12},
  number = {4},
  pages = {568},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  doi = {10.3390/math12040568},
  url = {https://www.mdpi.com/2227-7390/12/4/568},
  urldate = {2024-11-21},
  abstract = {Investigating public attitudes towards social media is crucial for opinion mining systems to gain valuable insights. Stance detection, which aims to discern the attitude expressed in an opinionated text towards a specific target, is a fundamental task in opinion mining. Conventional approaches mainly focus on sentence-level classification techniques. Recent research has shown that the integration of background knowledge can significantly improve stance detection performance. Despite the significant improvement achieved by knowledge-enhanced methods, applying these techniques in real-world scenarios remains challenging for several reasons. Firstly, existing methods often require the use of complex attention mechanisms to filter out noise and extract relevant background knowledge, which involves significant annotation efforts. Secondly, knowledge fusion mechanisms typically rely on fine-tuning, which can introduce a gap between the pre-training phase of pre-trained language models (PLMs) and the downstream stance detection tasks, leading to the poor prediction accuracy of the PLMs. To address these limitations, we propose a novel prompt-based stance detection method that leverages the knowledge acquired using the chain-of-thought method, which we refer to as PSDCOT. The proposed approach consists of two stages. The first stage is knowledge extraction, where instruction questions are constructed to elicit background knowledge from a VLPLM. The second stage is the multi-prompt learning network (M-PLN) for knowledge fusion, which learns model performance based on the background knowledge and the prompt learning framework. We evaluated the performance of PSDCOT on publicly available benchmark datasets to assess its effectiveness in improving stance detection performance. The results demonstrate that the proposed method achieves state-of-the-art results in in-domain, cross-target, and zero-shot learning settings.},
  issue = {4},
  langid = {english},
  keywords = {chain-of-thought,prompt-tuning,stance detection},
  file = {C:\Users\griswold\Zotero\storage\EMIF68HG\Ding et al. - 2024 - Leveraging Chain-of-Thought to Enhance Stance Dete.pdf}
}

@online{gaoMakingPretrainedLanguage2021b,
  title = {Making {{Pre-trained Language Models Better Few-shot Learners}}},
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  date = {2021-06-02},
  eprint = {2012.15723},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2012.15723},
  url = {http://arxiv.org/abs/2012.15723},
  urldate = {2024-11-20},
  abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\GCWZM7FC\\Gao et al. - 2021 - Making Pre-trained Language Models Better Few-shot.pdf;C\:\\Users\\griswold\\Zotero\\storage\\HIXXBE67\\2012.html}
}

@article{gilardiChatGPTOutperformsCrowd2023,
  title = {{{ChatGPT}} Outperforms Crowd Workers for Text-Annotation Tasks},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
  date = {2023-07-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {30},
  pages = {e2305016120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2305016120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2305016120},
  urldate = {2024-11-21},
  abstract = {Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.},
  file = {C:\Users\griswold\Zotero\storage\Y66KB78G\Gilardi et al. - 2023 - ChatGPT outperforms crowd workers for text-annotat.pdf}
}

@article{goverPoliticalBiasLarge2023,
  title = {Political {{Bias}} in {{Large Language Models}}},
  shorttitle = {Political {{Bias}} in {{Large Language Models}}},
  author = {Gover, Lucas},
  date = {2023},
  journaltitle = {The Commons},
  series = {The {{Commons}}: {{Puget Sound Journal}} of {{Politics}}. {{University}} of {{Puget Sound}}},
  volume = {4},
  number = {1},
  publisher = {The University of Puget Sound},
  url = {https://jstor.org/stable/community.36741697},
  urldate = {2024-12-11},
  file = {C:\Users\griswold\Zotero\storage\ZM5R8K8D\Gover - Political Bias in Large Language Models The Commo.pdf}
}

@online{gulStanceDetectionSocial2024,
  title = {Stance {{Detection}} on {{Social Media}} with {{Fine-Tuned Large Language Models}}},
  author = {Gül, İlker and Lebret, Rémi and Aberer, Karl},
  date = {2024-04-18},
  eprint = {2404.12171},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2404.12171},
  urldate = {2024-10-23},
  abstract = {Stance detection, a key task in natural language processing, determines an author's viewpoint based on textual analysis. This study evaluates the evolution of stance detection methods, transitioning from early machine learning approaches to the groundbreaking BERT model, and eventually to modern Large Language Models (LLMs) such as ChatGPT, LLaMa-2, and Mistral-7B. While ChatGPT's closed-source nature and associated costs present challenges, the open-source models like LLaMa-2 and Mistral-7B offers an encouraging alternative. Initially, our research focused on fine-tuning ChatGPT, LLaMa-2, and Mistral-7B using several publicly available datasets. Subsequently, to provide a comprehensive comparison, we assess the performance of these models in zero-shot and few-shot learning scenarios. The results underscore the exceptional ability of LLMs in accurately detecting stance, with all tested models surpassing existing benchmarks. Notably, LLaMa-2 and Mistral-7B demonstrate remarkable efficiency and potential for stance detection, despite their smaller sizes compared to ChatGPT. This study emphasizes the potential of LLMs in stance detection and calls for more extensive research in this field.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\ZA4IZECI\\Gül et al. - 2024 - Stance Detection on Social Media with Fine-Tuned L.pdf;C\:\\Users\\griswold\\Zotero\\storage\\4VHIQEXJ\\2404.html}
}

@inproceedings{maia2024enhancing,
  title={Enhancing Stance Detection in Low-Resource Brazilian Portuguese Using Corpus Expansion generated by GPT-3.5},
  author={Maia, Dyonnatan and da Silva, N{\'a}dia F{\'e}lix Felipe},
  booktitle={Proceedings of the 16th International Conference on Computational Processing of Portuguese},
  pages={503--508},
  year={2024}
}

@inproceedings{elkan2001foundations,
  title={The foundations of cost-sensitive learning},
  author={Elkan, Charles},
  booktitle={International joint conference on artificial intelligence},
  volume={17},
  number={1},
  pages={973--978},
  year={2001},
  organization={Lawrence Erlbaum Associates Ltd}
}

@article{hand2012assessing,
  title={Assessing the performance of classification methods},
  author={Hand, David J},
  journal={International Statistical Review},
  volume={80},
  number={3},
  pages={400--414},
  year={2012},
  publisher={Wiley Online Library}
}

@inproceedings{zadrozny2003cost,
  title={Cost-sensitive learning by cost-proportionate example weighting},
  author={Zadrozny, Bianca and Langford, John and Abe, Naoki},
  booktitle={Third IEEE international conference on data mining},
  pages={435--442},
  year={2003},
  organization={IEEE}
}

@online{heAnnoLLMMakingLarge2024,
  title = {{{AnnoLLM}}: {{Making Large Language Models}} to {{Be Better Crowdsourced Annotators}}},
  shorttitle = {{{AnnoLLM}}},
  author = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A.-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
  date = {2024-04-05},
  eprint = {2303.16854},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.16854},
  url = {http://arxiv.org/abs/2303.16854},
  urldate = {2024-11-21},
  abstract = {Many natural language processing (NLP) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt LLMs to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with LLMs. Our experiment results on three tasks, including user input and keyword relevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing AnnoLLM. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset's high quality.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\KJUFG454\\He et al. - 2024 - AnnoLLM Making Large Language Models to Be Better.pdf;C\:\\Users\\griswold\\Zotero\\storage\\9GYY8Y22\\2303.html}
}

@online{katoLuPINLLMbasedPolitical2024,
  title = {L(u){{PIN}}: {{LLM-based Political Ideology Nowcasting}}},
  shorttitle = {L(u){{PIN}}},
  author = {Kato, Ken and Purnomo, Annabelle and Cochrane, Christopher and Saqur, Raeid},
  date = {2024-05-12},
  eprint = {2405.07320},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.07320},
  url = {http://arxiv.org/abs/2405.07320},
  urldate = {2024-12-11},
  abstract = {The quantitative analysis of political ideological positions is a difficult task. In the past, various literature focused on parliamentary voting data of politicians, party manifestos and parliamentary speech to estimate political disagreement and polarization in various political systems. However previous methods of quantitative political analysis suffered from a common challenge which was the amount of data available for analysis. Also previous methods frequently focused on a more general analysis of politics such as overall polarization of the parliament or party-wide political ideological positions. In this paper, we present a method to analyze ideological positions of individual parliamentary representatives by leveraging the latent knowledge of LLMs. The method allows us to evaluate the stance of politicians on an axis of our choice allowing us to flexibly measure the stance of politicians in regards to a topic/controversy of our choice. We achieve this by using a fine-tuned BERT classifier to extract the opinion-based sentences from the speeches of representatives and projecting the average BERT embeddings for each representative on a pair of reference seeds. These reference seeds are either manually chosen representatives known to have opposing views on a particular topic or they are generated sentences which where created using the GPT-4 model of OpenAI. We created the sentences by prompting the GPT-4 model to generate a speech that would come from a politician defending a particular position.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\F7HQCAZ6\\Kato et al. - 2024 - L(u)PIN LLM-based Political Ideology Nowcasting.pdf;C\:\\Users\\griswold\\Zotero\\storage\\5EJHJFWW\\2405.html}
}

@inproceedings{kawintiranonKnowledgeEnhancedMasked2021,
  title = {Knowledge {{Enhanced Masked Language Model}} for {{Stance Detection}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Kawintiranon, Kornraphop and Singh, Lisa},
  editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  date = {2021-06},
  pages = {4725--4735},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.naacl-main.376},
  url = {https://aclanthology.org/2021.naacl-main.376},
  urldate = {2024-10-23},
  abstract = {Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Fine-tuned language models using large-scale in-domain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection. Instead of random token masking, we propose using a weighted log-odds-ratio to identify words with high stance distinguishability and then model an attention mechanism that focuses on these words. We show that our proposed approach outperforms the state of the art for stance detection on Twitter data about the 2020 US Presidential election.},
  eventtitle = {{{NAACL-HLT}} 2021},
  file = {C:\Users\griswold\Zotero\storage\J3Q8Q3QM\Kawintiranon and Singh - 2021 - Knowledge Enhanced Masked Language Model for Stanc.pdf}
}

@online{khiabaniCrossTargetStanceDetection2024,
  title = {Cross-{{Target Stance Detection}}: {{A Survey}} of {{Techniques}}, {{Datasets}}, and {{Challenges}}},
  shorttitle = {Cross-{{Target Stance Detection}}},
  author = {Khiabani, Parisa Jamadi and Zubiaga, Arkaitz},
  date = {2024-09-20},
  eprint = {2409.13594},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2409.13594},
  urldate = {2024-11-20},
  abstract = {Stance detection is the task of determining the viewpoint expressed in a text towards a given target. A specific direction within the task focuses on cross-target stance detection, where a model trained on samples pertaining to certain targets is then applied to a new, unseen target. With the increasing need to analyze and mining viewpoints and opinions online, the task has recently seen a significant surge in interest. This review paper examines the advancements in cross-target stance detection over the last decade, highlighting the evolution from basic statistical methods to contemporary neural and LLM-based models. These advancements have led to notable improvements in accuracy and adaptability. Innovative approaches include the use of topic-grouped attention and adversarial learning for zero-shot detection, as well as fine-tuning techniques that enhance model robustness. Additionally, prompt-tuning methods and the integration of external knowledge have further refined model performance. A comprehensive overview of the datasets used for evaluating these models is also provided, offering valuable insights into the progress and challenges in the field. We conclude by highlighting emerging directions of research and by suggesting avenues for future work in the task.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\UU6SXJNB\\Khiabani and Zubiaga - 2024 - Cross-Target Stance Detection A Survey of Techniq.pdf;C\:\\Users\\griswold\\Zotero\\storage\\JFEWJSSJ\\2409.html}
}

@online{kojimaLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  date = {2023-01-29},
  eprint = {2205.11916},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2205.11916},
  url = {http://arxiv.org/abs/2205.11916},
  urldate = {2024-11-21},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\9247XF8B\\Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf;C\:\\Users\\griswold\\Zotero\\storage\\TI4FDDBS\\2205.html}
}

@online{kristensen-mclachlanChatbotsAreNot2023,
  title = {Chatbots {{Are Not Reliable Text Annotators}}},
  author = {Kristensen-McLachlan, Ross Deans and Canavan, Miceal and Kardos, Márton and Jacobsen, Mia and Aarøe, Lene},
  date = {2023-11-09},
  eprint = {2311.05769},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2311.05769},
  url = {http://arxiv.org/abs/2311.05769},
  urldate = {2024-11-21},
  abstract = {Recent research highlights the significant potential of ChatGPT for text annotation in social science research. However, ChatGPT is a closed-source product which has major drawbacks with regards to transparency, reproducibility, cost, and data protection. Recent advances in open-source (OS) large language models (LLMs) offer alternatives which remedy these challenges. This means that it is important to evaluate the performance of OS LLMs relative to ChatGPT and standard approaches to supervised machine learning classification. We conduct a systematic comparative evaluation of the performance of a range of OS LLM models alongside ChatGPT, using both zero- and few-shot learning as well as generic and custom prompts, with results compared to more traditional supervised classification models. Using a new dataset of Tweets from US news media, and focusing on simple binary text annotation tasks for standard social science concepts, we find significant variation in the performance of ChatGPT and OS models across the tasks, and that supervised classifiers consistently outperform both. Given the unreliable performance of ChatGPT and the significant challenges it poses to Open Science we advise against using ChatGPT for substantive text annotation tasks in social science research.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\XGKZ7YGZ\\Kristensen-McLachlan et al. - 2023 - Chatbots Are Not Reliable Text Annotators.pdf;C\:\\Users\\griswold\\Zotero\\storage\\KDDYGI9H\\2311.html}
}

@online{laurerBuildingEfficientUniversal2024,
  title = {Building {{Efficient Universal Classifiers}} with {{Natural Language Inference}}},
  author = {Laurer, Moritz and family=Atteveldt, given=Wouter, prefix=van, useprefix=false and Casas, Andreu and Welbers, Kasper},
  date = {2024-03-22},
  eprint = {2312.17543},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.17543},
  url = {http://arxiv.org/abs/2312.17543},
  urldate = {2024-12-03},
  abstract = {Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier, and (3) shares the resulting universal classifier that is trained on 33 datasets with 389 diverse classes. Parts of the code we share has been used to train our older zeroshot classifiers that have been downloaded more than 55 million times via the Hugging Face Hub as of December 2023. Our new classifier improves zeroshot performance by 9.4\%.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\7EUSJMYY\\Laurer et al. - 2024 - Building Efficient Universal Classifiers with Natu.pdf;C\:\\Users\\griswold\\Zotero\\storage\\357HE72J\\2312.html}
}

@article{laurerLessAnnotatingMore2024a,
  title = {Less {{Annotating}}, {{More Classifying}}: {{Addressing}} the {{Data Scarcity Issue}} of {{Supervised Machine Learning}} with {{Deep Transfer Learning}} and {{BERT-NLI}}},
  shorttitle = {Less {{Annotating}}, {{More Classifying}}},
  author = {Laurer, Moritz and family=Atteveldt, given=Wouter, prefix=van, useprefix=false and Casas, Andreu and Welbers, Kasper},
  date = {2024-01},
  journaltitle = {Political Analysis},
  volume = {32},
  number = {1},
  pages = {84--100},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2023.20},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/less-annotating-more-classifying-addressing-the-data-scarcity-issue-of-supervised-machine-learning-with-deep-transfer-learning-and-bertnli/05BB05555241762889825B080E097C27},
  urldate = {2024-12-03},
  abstract = {Supervised machine learning is an increasingly popular tool for analyzing large political text corpora. The main disadvantage of supervised machine learning is the need for thousands of manually annotated training data points. This issue is particularly important in the social sciences where most new research questions require new training data for a new task tailored to the specific research question. This paper analyses how deep transfer learning can help address this challenge by accumulating “prior knowledge” in language models. Models like BERT can learn statistical language patterns through pre-training (“language knowledge”), and reliance on task-specific data can be reduced by training on universal tasks like natural language inference (NLI; “task knowledge”). We demonstrate the benefits of transfer learning on a wide range of eight tasks. Across these eight tasks, our BERT-NLI model fine-tuned on 100 to 2,500 texts performs on average 10.7 to 18.3 percentage points better than classical models without transfer learning. Our study indicates that BERT-NLI fine-tuned on 500 texts achieves similar performance as classical models trained on around 5,000 texts. Moreover, we show that transfer learning works particularly well on imbalanced data. We conclude by discussing limitations of transfer learning and by outlining new opportunities for political science research.},
  langid = {english},
  keywords = {computational methods,machine learning,text as data,transfer learning},
  file = {C:\Users\griswold\Zotero\storage\LAAQ6EJT\Laurer et al. - 2024 - Less Annotating, More Classifying Addressing the .pdf}
}

@online{liAdvancingAnnotationStance2024,
  title = {Advancing {{Annotation}} of {{Stance}} in {{Social Media Posts}}: {{A Comparative Analysis}} of {{Large Language Models}} and {{Crowd Sourcing}}},
  shorttitle = {Advancing {{Annotation}} of {{Stance}} in {{Social Media Posts}}},
  author = {Li, Mao and Conrad, Frederick},
  date = {2024-06-11},
  eprint = {2406.07483},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2406.07483},
  urldate = {2024-10-23},
  abstract = {In the rapidly evolving landscape of Natural Language Processing (NLP), the use of Large Language Models (LLMs) for automated text annotation in social media posts has garnered significant interest. Despite the impressive innovations in developing LLMs like ChatGPT, their efficacy, and accuracy as annotation tools are not well understood. In this paper, we analyze the performance of eight open-source and proprietary LLMs for annotating the stance expressed in social media posts, benchmarking their performance against human annotators' (i.e., crowd-sourced) judgments. Additionally, we investigate the conditions under which LLMs are likely to disagree with human judgment. A significant finding of our study is that the explicitness of text expressing a stance plays a critical role in how faithfully LLMs' stance judgments match humans'. We argue that LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement. We conclude with recommendations for a comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions. This study highlights the importance of improving the accuracy and comprehensiveness of automated stance detection, aiming to advance these technologies for more efficient and unbiased analysis of social media.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\593EKBAG\\Li and Conrad - 2024 - Advancing Annotation of Stance in Social Media Pos.pdf;C\:\\Users\\griswold\\Zotero\\storage\\49TQ3PUQ\\2406.html}
}

@inproceedings{liangTargetadaptiveGraphCrosstarget2021,
  title = {Target-Adaptive {{Graph}} for {{Cross-target Stance Detection}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Liang, Bin and Fu, Yonghao and Gui, Lin and Yang, Min and Du, Jiachen and He, Yulan and Xu, Ruifeng},
  date = {2021-06-03},
  series = {{{WWW}} '21},
  pages = {3453--3464},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3442381.3449790},
  url = {https://dl.acm.org/doi/10.1145/3442381.3449790},
  urldate = {2024-11-21},
  abstract = {Target plays an essential role in stance detection of an opinionated review/claim, since the stance expressed in the text often depends on the target. In practice, we need to deal with targets unseen in the annotated training data. As such, detecting stance for an unknown or unseen target is an important research problem. This paper presents a novel approach that automatically identifies and adapts the target-dependent and target-independent roles that a word plays with respect to a specific target in stance expressions, so as to achieve cross-target stance detection. More concretely, we explore a novel solution of constructing heterogeneous target-adaptive pragmatics dependency graphs (TPDG) for each sentence towards a given target. An in-target graph is constructed to produce inherent pragmatics dependencies of words for a distinct target. In addition, another cross-target graph is constructed to develop the versatility of words across all targets for boosting the learning of dominant word-level stance expressions available to an unknown target. A novel graph-aware model with interactive Graphical Convolutional Network (GCN) blocks is developed to derive the target-adaptive graph representation of the context for stance detection. The experimental results on a number of benchmark datasets show that our proposed model outperforms state-of-the-art methods in cross-target stance detection.},
  isbn = {978-1-4503-8312-7},
  file = {C:\Users\griswold\Zotero\storage\L9NRTVYA\Liang et al. - 2021 - Target-adaptive Graph for Cross-target Stance Dete.pdf}
}

@article{lauderdale2016measuring,
  title={Measuring political positions from legislative speech},
  author={Lauderdale, Benjamin E and Herzog, Alexander},
  journal={Political Analysis},
  volume={24},
  number={3},
  pages={374--394},
  year={2016},
  publisher={Cambridge University Press}
}

@inproceedings{yan2019congressional,
  title={The congressional classification challenge: Domain specificity and partisan intensity},
  author={Yan, Hao and Das, Sanmay and Lavoie, Allen and Li, Sirui and Sinclair, Betsy},
  booktitle={Proceedings of the 2019 ACM Conference on Economics and Computation},
  pages={71--89},
  year={2019}
}
@article{burscher2015using,
  title={Using supervised machine learning to code policy issues: Can classifiers generalize across contexts?},
  author={Burscher, Bjorn and Vliegenthart, Rens and De Vreese, Claes H},
  journal={The ANNALS of the American Academy of Political and Social Science},
  volume={659},
  number={1},
  pages={122--131},
  year={2015},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@inproceedings{liPStanceLargeDataset2021,
  title = {P-{{Stance}}: {{A Large Dataset}} for {{Stance Detection}} in {{Political Domain}}},
  shorttitle = {P-{{Stance}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Li, Yingjie and Sosea, Tiberiu and Sawant, Aditya and Nair, Ajith Jayaraman and Inkpen, Diana and Caragea, Cornelia},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  date = {2021-08},
  pages = {2355--2365},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.findings-acl.208},
  url = {https://aclanthology.org/2021.findings-acl.208},
  urldate = {2024-10-23},
  eventtitle = {Findings 2021},
  file = {C:\Users\griswold\Zotero\storage\VS8AGNVZ\Li et al. - 2021 - P-Stance A Large Dataset for Stance Detection in .pdf}
}

@online{maChainStanceStance2024a,
  title = {Chain of {{Stance}}: {{Stance Detection}} with {{Large Language Models}}},
  shorttitle = {Chain of {{Stance}}},
  author = {Ma, Junxia and Wang, Changjiang and Xing, Hanwen and Zhao, Dongming and Zhang, Yazhou},
  date = {2024-08-03},
  eprint = {2408.04649},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2408.04649},
  urldate = {2024-11-20},
  abstract = {Stance detection is an active task in natural language processing (NLP) that aims to identify the author's stance towards a particular target within a text. Given the remarkable language understanding capabilities and encyclopedic prior knowledge of large language models (LLMs), how to explore the potential of LLMs in stance detection has received significant attention. Unlike existing LLM-based approaches that focus solely on fine-tuning with large-scale datasets, we propose a new prompting method, called \textbackslash textit\{Chain of Stance\} (CoS). In particular, it positions LLMs as expert stance detectors by decomposing the stance detection process into a series of intermediate, stance-related assertions that culminate in the final judgment. This approach leads to significant improvements in classification performance. We conducted extensive experiments using four SOTA LLMs on the SemEval 2016 dataset, covering the zero-shot and few-shot learning setups. The results indicate that the proposed method achieves state-of-the-art results with an F1 score of 79.84 in the few-shot setting.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\6L2RRG8E\\Ma et al. - 2024 - Chain of Stance Stance Detection with Large Langu.pdf;C\:\\Users\\griswold\\Zotero\\storage\\P6H353SA\\2408.html}
}

@article{vamvas2020x,
  title={X-stance: A multilingual multi-target dataset for stance detection},
  author={Vamvas, Jannis and Sennrich, Rico},
  journal={arXiv preprint arXiv:2003.08385},
  year={2020}
}

@inproceedings{lan2024stance,
  title={Stance detection with collaborative role-infused llm-based agents},
  author={Lan, Xiaochong and Gao, Chen and Jin, Depeng and Li, Yong},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={18},
  pages={891--903},
  year={2024}
}

@article{khiabani2024cross,
  title={Cross-Target Stance Detection: A Survey of Techniques, Datasets, and Challenges},
  author={Khiabani, Parisa Jamadi and Zubiaga, Arkaitz},
  journal={arXiv preprint arXiv:2409.13594},
  year={2024}
}

@article{poole1985spatial,
  title={A spatial model for legislative roll call analysis},
  author={Poole, Keith T and Rosenthal, Howard},
  journal={American journal of political science},
  pages={357--384},
  year={1985},
  publisher={JSTOR}
}

@article{osnabrugge2023cross,
  title={Cross-domain topic classification for political texts},
  author={Osnabr{\"u}gge, Moritz and Ash, Elliott and Morelli, Massimo},
  journal={Political Analysis},
  volume={31},
  number={1},
  pages={59--80},
  year={2023},
  publisher={Cambridge University Press}
}

@article{carroll2009measuring,
  title={Measuring bias and uncertainty in DW-NOMINATE ideal point estimates via the parametric bootstrap},
  author={Carroll, Royce and Lewis, Jeffrey B and Lo, James and Poole, Keith T and Rosenthal, Howard},
  journal={Political analysis},
  volume={17},
  number={3},
  pages={261--275},
  year={2009},
  publisher={Cambridge University Press}
}

@article{schiller2021stance,
  title={Stance detection benchmark: How robust is your stance detection?},
  author={Schiller, Benjamin and Daxenberger, Johannes and Gurevych, Iryna},
  journal={KI-K{\"u}nstliche Intelligenz},
  pages={1--13},
  year={2021},
  publisher={Springer}
}

@article{bergam2022legal,
  title={Legal and political stance detection of SCOTUS language},
  author={Bergam, Noah and Allaway, Emily and Mckeown, Kathleen},
  journal={arXiv preprint arXiv:2211.11724},
  year={2022}
}

@online{mensPositioningPoliticalTexts2024,
  title = {Positioning {{Political Texts}} with {{Large Language Models}} by {{Asking}} and {{Averaging}}},
  author = {Mens, Gaël Le and Gallego, Aina},
  date = {2024-09-05},
  eprint = {2311.16639},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2311.16639},
  url = {http://arxiv.org/abs/2311.16639},
  urldate = {2024-11-22},
  abstract = {We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3, MiXtral, or Aya to position political texts within policy and ideological spaces. We ask an LLM where a tweet or a sentence of a political text stands on the focal dimension and take the average of the LLM responses to position political actors such as US Senators, or longer texts such as UK party manifestos or EU policy speeches given in 10 different languages. The correlations between the position estimates obtained with the best LLMs and benchmarks based on text coding by experts, crowdworkers, or roll call votes exceed .90. This approach is generally more accurate than the positions obtained with supervised classifiers trained on large amounts of research data. Using instruction-tuned LLMs to position texts in policy and ideological spaces is fast, cost-efficient, reliable, and reproducible (in the case of open LLMs) even if the texts are short and written in different languages. We conclude with cautionary notes about the need for empirical validation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\2G7FJLAN\\Mens and Gallego - 2024 - Positioning Political Texts with Large Language Mo.pdf;C\:\\Users\\griswold\\Zotero\\storage\\AV5PPEQ9\\2311.html}
}

@article{metsAutomatedStanceDetection2024,
  title = {Automated Stance Detection in Complex Topics and Small Languages: {{The}} Challenging Case of Immigration in Polarizing News Media},
  shorttitle = {Automated Stance Detection in Complex Topics and Small Languages},
  author = {Mets, Mark and Karjus, Andres and Ibrus, Indrek and Schich, Maximilian},
  date = {2024-04-26},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {19},
  number = {4},
  pages = {e0302380},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0302380},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0302380},
  urldate = {2024-11-20},
  abstract = {Automated stance detection and related machine learning methods can provide useful insights for media monitoring and academic research. Many of these approaches require annotated training datasets, which limits their applicability for languages where these may not be readily available. This paper explores the applicability of large language models for automated stance detection in a challenging scenario, involving a morphologically complex, lower-resource language, and a socio-culturally complex topic, immigration. If the approach works in this case, it can be expected to perform as well or better in less demanding scenarios. We annotate a large set of pro- and anti-immigration examples to train and compare the performance of multiple language models. We also probe the usability of GPT-3.5 (that powers ChatGPT) as an instructable zero-shot classifier for the same task. The supervised models achieve acceptable performance, but GPT-3.5 yields similar accuracy. As the latter does not require tuning with annotated data, it constitutes a potentially simpler and cheaper alternative for text classification tasks, including in lower-resource languages. We further use the best-performing supervised model to investigate diachronic trends over seven years in two corpora of Estonian mainstream and right-wing populist news sources, demonstrating the applicability of automated stance detection for news analytics and media monitoring settings even in lower-resource scenarios, and discuss correspondences between stance changes and real-world events.},
  langid = {english},
  keywords = {Culture,Elections,Estonia,Europe,Languages,Multilingualism,Natural language processing,Refugees},
  file = {C:\Users\griswold\Zotero\storage\4IH48ENZ\Mets et al. - 2024 - Automated stance detection in complex topics and s.pdf}
}

@article{motokiMoreHumanHuman2024,
  title = {More Human than Human: Measuring {{ChatGPT}} Political Bias},
  shorttitle = {More Human than Human},
  author = {Motoki, Fabio and Pinho Neto, Valdemar and Rodrigues, Victor},
  date = {2024-01-01},
  journaltitle = {Public Choice},
  shortjournal = {Public Choice},
  volume = {198},
  number = {1},
  pages = {3--23},
  issn = {1573-7101},
  doi = {10.1007/s11127-023-01097-2},
  url = {https://doi.org/10.1007/s11127-023-01097-2},
  urldate = {2024-11-22},
  abstract = {We investigate the political bias of a large language model (LLM), ChatGPT, which has become popular for retrieving factual information and generating content. Although ChatGPT assures that it is impartial, the literature suggests that LLMs exhibit bias involving race, gender, religion, and political orientation. Political bias in LLMs can have adverse political and electoral consequences similar to bias from traditional and social media. Moreover, political bias can be harder to detect and eradicate than gender or racial bias. We propose a novel empirical design to infer whether ChatGPT has political biases by requesting it to impersonate someone from a given side of the political spectrum and comparing these answers with its default. We also propose dose-response, placebo, and profession-politics alignment robustness tests. To reduce concerns about the randomness of the generated text, we collect answers to the same questions 100 times, with question order randomized on each round. We find robust evidence that ChatGPT presents a significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK. These results translate into real concerns that ChatGPT, and LLMs in general, can extend or even amplify the existing challenges involving political processes posed by the Internet and social media. Our findings have important implications for policymakers, media, politics, and academia stakeholders.},
  langid = {english},
  keywords = {Bias,C10,C89,ChatGPT,D83,L86,Large language models,Political bias,Z00},
  file = {C:\Users\griswold\Zotero\storage\YABXM26P\Motoki et al. - 2024 - More human than human measuring ChatGPT political.pdf}
}

@online{muNavigatingPromptComplexity2024,
  title = {Navigating {{Prompt Complexity}} for {{Zero-Shot Classification}}: {{A Study}} of {{Large Language Models}} in {{Computational Social Science}}},
  shorttitle = {Navigating {{Prompt Complexity}} for {{Zero-Shot Classification}}},
  author = {Mu, Yida and Wu, Ben P. and Thorne, William and Robinson, Ambrose and Aletras, Nikolaos and Scarton, Carolina and Bontcheva, Kalina and Song, Xingyi},
  date = {2024-03-24},
  eprint = {2305.14310},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2305.14310},
  url = {http://arxiv.org/abs/2305.14310},
  urldate = {2024-11-22},
  abstract = {Instruction-tuned Large Language Models (LLMs) have exhibited impressive language understanding and the capacity to generate responses that follow specific prompts. However, due to the computational demands associated with training these models, their applications often adopt a zero-shot setting. In this paper, we evaluate the zero-shot performance of two publicly accessible LLMs, ChatGPT and OpenAssistant, in the context of six Computational Social Science classification tasks, while also investigating the effects of various prompting strategies. Our experiments investigate the impact of prompt complexity, including the effect of incorporating label definitions into the prompt; use of synonyms for label names; and the influence of integrating past memories during foundation model training. The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large). Additionally, we find that different prompting strategies can significantly affect classification accuracy, with variations in accuracy and F1 scores exceeding 10\textbackslash\%.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\M4I8MGRP\\Mu et al. - 2024 - Navigating Prompt Complexity for Zero-Shot Classif.pdf;C\:\\Users\\griswold\\Zotero\\storage\\T2KEQ6QR\\2305.html}
}

@online{ngExaminingInfluencePolitical2024,
  title = {Examining the {{Influence}} of {{Political Bias}} on {{Large Language Model Performance}} in {{Stance Classification}}},
  author = {Ng, Lynnette Hui Xian and Cruickshank, Iain and Lee, Roy Ka-Wei},
  date = {2024-07-26},
  eprint = {2407.17688},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2407.17688},
  url = {http://arxiv.org/abs/2407.17688},
  urldate = {2024-11-22},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in executing tasks based on natural language queries. However, these models, trained on curated datasets, inherently embody biases ranging from racial to national and gender biases. It remains uncertain whether these biases impact the performance of LLMs for certain tasks. In this study, we investigate the political biases of LLMs within the stance classification task, specifically examining whether these models exhibit a tendency to more accurately classify politically-charged stances. Utilizing three datasets, seven LLMs, and four distinct prompting schemes, we analyze the performance of LLMs on politically oriented statements and targets. Our findings reveal a statistically significant difference in the performance of LLMs across various politically oriented stance classification tasks. Furthermore, we observe that this difference primarily manifests at the dataset level, with models and prompting schemes showing statistically similar performances across different stance classification datasets. Lastly, we observe that when there is greater ambiguity in the target the statement is directed towards, LLMs have poorer stance classification accuracy. Code \& Dataset: http://doi.org/10.5281/zenodo.12938478},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\YXT9LJQ3\\Ng et al. - 2024 - Examining the Influence of Political Bias on Large.pdf;C\:\\Users\\griswold\\Zotero\\storage\\MHK9BS32\\2407.html}
}

@article{ngMyStanceSame2022,
  title = {Is My Stance the Same as Your Stance? {{A}} Cross Validation Study of Stance Detection Datasets},
  shorttitle = {Is My Stance the Same as Your Stance?},
  author = {Ng, Lynnette Hui Xian and Carley, Kathleen M.},
  date = {2022-11-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {59},
  number = {6},
  pages = {103070},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2022.103070},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457322001728},
  urldate = {2024-11-22},
  abstract = {Stance detection identifies a person’s evaluation of a subject, and is a crucial component for many downstream applications. In application, stance detection requires training a machine learning model on an annotated dataset and applying the model on another to predict stances of text snippets. This cross-dataset model generalization poses three central questions, which we investigate using stance classification models on 7 publicly available English Twitter datasets ranging from 297 to 48,284 instances. (1) Are stance classification models generalizable across datasets? We construct a single dataset model to train/test dataset-against-dataset, finding models do not generalize well (avg F1=0.33). (2) Can we improve the generalizability by aggregating datasets? We find a multi dataset model built on the aggregation of datasets has an improved performance (avg F1=0.69). (3) Given a model built on multiple datasets, how much additional data is required to fine-tune it? We find it challenging to ascertain a minimum number of data points due to the lack of pattern in performance. Investigating possible reasons for the choppy model performance we find that texts are not easily differentiable by stances, nor are annotations consistent within and across datasets. Our observations emphasize the need for an aggregated dataset as well as consistent labels for the generalizability of models.},
  keywords = {Cross validation,Machine learning,Natural language processing,Stance detection,Twitter},
  file = {C:\Users\griswold\Zotero\storage\86WXDY6B\S0306457322001728.html}
}

@article{osnabruggeCrossDomainTopicClassification2023,
  title = {Cross-{{Domain Topic Classification}} for {{Political Texts}}},
  author = {Osnabrügge, Moritz and Ash, Elliott and Morelli, Massimo},
  date = {2023-01},
  journaltitle = {Political Analysis},
  volume = {31},
  number = {1},
  pages = {59--80},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2021.37},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/crossdomain-topic-classification-for-political-texts/F074564984969CE168BCBCF5E7D931C8},
  urldate = {2024-11-21},
  abstract = {We introduce and assess the use of supervised learning in cross-domain topic classification. In this approach, an algorithm learns to classify topics in a labeled source corpus and then extrapolates topics in an unlabeled target corpus from another domain. The ability to use existing training data makes this method significantly more efficient than within-domain supervised learning. It also has three advantages over unsupervised topic models: the method can be more specifically targeted to a research question and the resulting topics are easier to validate and interpret. We demonstrate the method using the case of labeled party platforms (source corpus) and unlabeled parliamentary speeches (target corpus). In addition to the standard within-domain error metrics, we further validate the cross-domain performance by labeling a subset of target-corpus documents. We find that the classifier accurately assigns topics in the parliamentary speeches, although accuracy varies substantially by topic. We also propose tools diagnosing cross-domain classification. To illustrate the usefulness of the method, we present two case studies on how electoral rules and the gender of parliamentarians influence the choice of speech topics.},
  langid = {english},
  keywords = {cross-domain classification,debate participation,electoral reform,manifesto corpus,parliamentary speeches,supervised learning,text analysis},
  file = {C:\Users\griswold\Zotero\storage\YEAVZH24\Osnabrügge et al. - 2023 - Cross-Domain Topic Classification for Political Te.pdf}
}

@online{perezTrueFewShotLearning2021,
  title = {True {{Few-Shot Learning}} with {{Language Models}}},
  author = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  date = {2021-05-24},
  eprint = {2105.11447},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2105.11447},
  url = {http://arxiv.org/abs/2105.11447},
  urldate = {2024-11-21},
  abstract = {Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates ("prompts"). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\C4LIMA7V\\Perez et al. - 2021 - True Few-Shot Learning with Language Models.pdf;C\:\\Users\\griswold\\Zotero\\storage\\KN65WZ8F\\2105.html}
}

@online{rozadoPoliticalPreferencesLLMs2024,
  title = {The {{Political Preferences}} of {{LLMs}}},
  author = {Rozado, David},
  date = {2024-06-02},
  eprint = {2402.01789},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2402.01789},
  url = {http://arxiv.org/abs/2402.01789},
  urldate = {2024-11-22},
  abstract = {I report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, I administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both closed and open source. When probed with questions/statements with political connotations, most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. This does not appear to be the case for five additional base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, the weak performance of the base models at coherently answering the tests' questions makes this subset of results inconclusive. Finally, I demonstrate that LLMs can be steered towards specific locations in the political spectrum through Supervised Fine-Tuning (SFT) with only modest amounts of politically aligned data, suggesting SFT's potential to embed political orientation in LLMs. With LLMs beginning to partially displace traditional information sources like search engines and Wikipedia, the societal implications of political biases embedded in LLMs are substantial.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\FFVF55QL\\Rozado - 2024 - The Political Preferences of LLMs.pdf;C\:\\Users\\griswold\\Zotero\\storage\\XLBIMMCZ\\2402.html}
}

@article{schillerStanceDetectionBenchmark2021a,
  title = {Stance {{Detection Benchmark}}: {{How Robust}} Is {{Your Stance Detection}}?},
  shorttitle = {Stance {{Detection Benchmark}}},
  author = {Schiller, Benjamin and Daxenberger, Johannes and Gurevych, Iryna},
  date = {2021-11-01},
  journaltitle = {KI - Künstliche Intelligenz},
  shortjournal = {Künstl Intell},
  volume = {35},
  number = {3},
  pages = {329--341},
  issn = {1610-1987},
  doi = {10.1007/s13218-021-00714-w},
  url = {https://doi.org/10.1007/s13218-021-00714-w},
  urldate = {2024-10-23},
  abstract = {Stance detection (StD) aims to detect an author’s stance towards a certain topic and has become a key component in applications like fake news detection, claim validation, or argument search. However, while stance is easily detected by humans, machine learning (ML) models are clearly falling short of this task. Given the major differences in dataset sizes and framing of StD (e.g. number of classes and inputs), ML models trained on a single dataset usually generalize poorly to other domains. Hence, we introduce a StD benchmark that allows to compare ML models against a wide variety of heterogeneous StD datasets to evaluate them for generalizability and robustness. Moreover, the framework is designed for easy integration of new datasets and probing methods for robustness. Amongst several baseline models, we define a model that learns from all ten StD datasets of various domains in a multi-dataset learning (MDL) setting and present new state-of-the-art results on five of the datasets. Yet, the models still perform well below human capabilities and even simple perturbations of the original test samples (adversarial attacks) severely hurt the performance of MDL models. Deeper investigation suggests overfitting on dataset biases as the main reason for the decreased robustness. Our analysis emphasizes the need of focus on robustness and de-biasing strategies in multi-task learning approaches. To foster research on this important topic, we release the dataset splits, code, and fine-tuned weights.},
  langid = {english},
  keywords = {Artificial Intelligence,Multi-dataset learning,Robustness,Stance detection},
  file = {C:\Users\griswold\Zotero\storage\BKYY9FZ3\Schiller et al. - 2021 - Stance Detection Benchmark How Robust is Your Sta.pdf}
}

@online{sharmaArgumentativeStancePrediction2023,
  title = {Argumentative {{Stance Prediction}}: {{An Exploratory Study}} on {{Multimodality}} and {{Few-Shot Learning}}},
  shorttitle = {Argumentative {{Stance Prediction}}},
  author = {Sharma, Arushi and Gupta, Abhibha and Bilalpur, Maneesh},
  date = {2023-10-11},
  eprint = {2310.07093},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2310.07093},
  urldate = {2024-11-20},
  abstract = {To advance argumentative stance prediction as a multimodal problem, the First Shared Task in Multimodal Argument Mining hosted stance prediction in crucial social topics of gun control and abortion. Our exploratory study attempts to evaluate the necessity of images for stance prediction in tweets and compare out-of-the-box text-based large-language models (LLM) in few-shot settings against fine-tuned unimodal and multimodal models. Our work suggests an ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms both the multimodal (0.677 F1-score) and text-based few-shot prediction using a recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in performance, our findings suggest that the multimodal models tend to perform better when image content is summarized as natural language over their native pixel structure and, using in-context examples improves few-shot performance of LLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\LL295TIB\\Sharma et al. - 2023 - Argumentative Stance Prediction An Exploratory St.pdf;C\:\\Users\\griswold\\Zotero\\storage\\SV98WLR9\\2310.html}
}

@online{tornbergChatGPT4OutperformsExperts2023,
  title = {{{ChatGPT-4 Outperforms Experts}} and {{Crowd Workers}} in {{Annotating Political Twitter Messages}} with {{Zero-Shot Learning}}},
  author = {Törnberg, Petter},
  date = {2023-04-13},
  eprint = {2304.06588},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2304.06588},
  url = {http://arxiv.org/abs/2304.06588},
  urldate = {2024-11-21},
  abstract = {This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\J8RCCUD6\\Törnberg - 2023 - ChatGPT-4 Outperforms Experts and Crowd Workers in.pdf;C\:\\Users\\griswold\\Zotero\\storage\\8274UTVA\\2304.html}
}

@online{vatsalSurveyPromptEngineering2024,
  title = {A {{Survey}} of {{Prompt Engineering Methods}} in {{Large Language Models}} for {{Different NLP Tasks}}},
  author = {Vatsal, Shubham and Dubey, Harsh},
  date = {2024-07-24},
  eprint = {2407.12994},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2407.12994},
  urldate = {2024-10-23},
  abstract = {Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs' knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\5R565557\\Vatsal and Dubey - 2024 - A Survey of Prompt Engineering Methods in Large La.pdf;C\:\\Users\\griswold\\Zotero\\storage\\NZVNIZJR\\2407.html}
}

@article{weiChainofThoughtPromptingElicits2022,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {24824--24837},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  urldate = {2024-11-21},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\ZWDBP588\Wei et al. - 2022 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf}
}

@online{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2024-11-27},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\UFCT5KSX\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf;C\:\\Users\\griswold\\Zotero\\storage\\WZPJLVTW\\2201.html}
}

@inproceedings{weiModelingTransferableTopics2019,
  title = {Modeling {{Transferable Topics}} for {{Cross-Target Stance Detection}}},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Wei, Penghui and Mao, Wenji},
  date = {2019-07-18},
  series = {{{SIGIR}}'19},
  pages = {1173--1176},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3331184.3331367},
  url = {https://dl.acm.org/doi/10.1145/3331184.3331367},
  urldate = {2024-11-21},
  abstract = {Targeted stance detection aims to classify the attitude of an opinionated text towards a pre-defined target. Previous methods mainly focus on in-target setting that models are trained and tested using data specific to the same target. In practical cases, the target we concern may have few or no labeled data, which restrains us from training a target-specific model. In this paper we study the problem of cross-target stance detection, utilizing labeled data of a source target to learn models that can be adapted to a destination target. To this end, we propose an effective method, the core intuition of which is to leverage shared latent topics between two targets as transferable knowledge to facilitate model adaptation. Our method acquires topic knowledge with neural variational inference, and further adopts adversarial training that encourages the model to learn target-invariant representations. Experimental results verify that our proposed method is superior to the state-of-the-art methods.},
  isbn = {978-1-4503-6172-9},
  file = {C:\Users\griswold\Zotero\storage\GBULTX2N\Wei and Mao - 2019 - Modeling Transferable Topics for Cross-Target Stan.pdf}
}

@inproceedings{wenZeroShotFewShotStance2023,
  title = {Zero-{{Shot}} and {{Few-Shot Stance Detection}} on {{Varied Topics}} via {{Conditional Generation}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Wen, Haoyang and Hauptmann, Alexander},
  editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  date = {2023-07},
  pages = {1491--1499},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-short.127},
  url = {https://aclanthology.org/2023.acl-short.127},
  urldate = {2024-11-20},
  abstract = {Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.},
  eventtitle = {{{ACL}} 2023},
  file = {C:\Users\griswold\Zotero\storage\FAX3MMVG\Wen and Hauptmann - 2023 - Zero-Shot and Few-Shot Stance Detection on Varied .pdf}
}

@article{widmannCreatingComparingDictionary2023,
  title = {Creating and {{Comparing Dictionary}}, {{Word Embedding}}, and {{Transformer-Based Models}} to {{Measure Discrete Emotions}} in {{German Political Text}}},
  author = {Widmann, Tobias and Wich, Maximilian},
  date = {2023-10},
  journaltitle = {Political Analysis},
  volume = {31},
  number = {4},
  pages = {626--641},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2022.15},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/creating-and-comparing-dictionary-word-embedding-and-transformerbased-models-to-measure-discrete-emotions-in-german-political-text/2DA41C0F09DE1CA600B3DCC647302637},
  urldate = {2024-10-23},
  abstract = {Previous research on emotional language relied heavily on off-the-shelf sentiment dictionaries that focus on negative and positive tone. These dictionaries are often tailored to nonpolitical domains and use bag-of-words approaches which come with a series of disadvantages. This paper creates, validates, and compares the performance of (1) a novel emotional dictionary specifically for political text, (2) locally trained word embedding models combined with simple neural network classifiers, and (3) transformer-based models which overcome limitations of the dictionary approach. All tools can measure emotional appeals associated with eight discrete emotions. The different approaches are validated on different sets of crowd-coded sentences. Encouragingly, the results highlight the strengths of novel transformer-based models, which come with easily available pretrained language models. Furthermore, all customized approaches outperform widely used off-the-shelf dictionaries in measuring emotional language in German political discourse.},
  langid = {english},
  keywords = {dictionary,emotions,political text,text-as-data,transformer models,word embeddings},
  file = {C:\Users\griswold\Zotero\storage\8MC622MD\Widmann and Wich - 2023 - Creating and Comparing Dictionary, Word Embedding,.pdf}
}

@online{wuLargeLanguageModels2023,
  title = {Large {{Language Models Can Be Used}} to {{Estimate}} the {{Latent Positions}} of {{Politicians}}},
  author = {Wu, Patrick Y. and Nagler, Jonathan and Tucker, Joshua A. and Messing, Solomon},
  date = {2023-09-26},
  eprint = {2303.12057},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2303.12057},
  urldate = {2024-10-23},
  abstract = {Existing approaches to estimating politicians' latent positions along specific dimensions often fail when relevant data is limited. We leverage the embedded knowledge in generative large language models (LLMs) to address this challenge and measure lawmakers' positions along specific political or policy dimensions. We prompt an instruction/dialogue-tuned LLM to pairwise compare lawmakers and then scale the resulting graph using the Bradley-Terry model. We estimate novel measures of U.S. senators' positions on liberal-conservative ideology, gun control, and abortion. Our liberal-conservative scale, used to validate LLM-driven scaling, strongly correlates with existing measures and offsets interpretive gaps, suggesting LLMs synthesize relevant data from internet and digitized media rather than memorizing existing measures. Our gun control and abortion measures -- the first of their kind -- differ from the liberal-conservative scale in face-valid ways and predict interest group ratings and legislator votes better than ideology alone. Our findings suggest LLMs hold promise for solving complex social science measurement problems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\YFBKTVB5\\Wu et al. - 2023 - Large Language Models Can Be Used to Estimate the .pdf;C\:\\Users\\griswold\\Zotero\\storage\\M9WUKYNG\\2303.html}
}

@inproceedings{xuCrossTargetStanceClassification2018,
  title = {Cross-{{Target Stance Classification}} with {{Self-Attention Networks}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Xu, Chang and Paris, Cécile and Nepal, Surya and Sparks, Ross},
  editor = {Gurevych, Iryna and Miyao, Yusuke},
  date = {2018-07},
  pages = {778--783},
  publisher = {Association for Computational Linguistics},
  location = {Melbourne, Australia},
  doi = {10.18653/v1/P18-2123},
  url = {https://aclanthology.org/P18-2123},
  urldate = {2024-11-22},
  abstract = {In stance classification, the target on which the stance is made defines the boundary of the task, and a classifier is usually trained for prediction on the same target. In this work, we explore the potential for generalizing classifiers between different targets, and propose a neural model that can apply what has been learned from a source target to a destination target. We show that our model can find useful information shared between relevant targets which improves generalization in certain scenarios.},
  eventtitle = {{{ACL}} 2018},
  file = {C:\Users\griswold\Zotero\storage\SWTSIR2J\Xu et al. - 2018 - Cross-Target Stance Classification with Self-Atten.pdf}
}

@online{zarrellaMITRESemEval2016Task2016,
  title = {{{MITRE}} at {{SemEval-2016 Task}} 6: {{Transfer Learning}} for {{Stance Detection}}},
  shorttitle = {{{MITRE}} at {{SemEval-2016 Task}} 6},
  author = {Zarrella, Guido and Marsh, Amy},
  date = {2016-06-13},
  eprint = {1606.03784},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1606.03784},
  url = {http://arxiv.org/abs/1606.03784},
  urldate = {2024-10-23},
  abstract = {We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance in Tweets. This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic. We employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets. We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task. These sentence vectors were then fine-tuned for stance detection on several hundred labeled examples. The result was a high performing system that used transfer learning to maximize the value of the available training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\VSDKU675\\Zarrella and Marsh - 2016 - MITRE at SemEval-2016 Task 6 Transfer Learning fo.pdf;C\:\\Users\\griswold\\Zotero\\storage\\HUP9TZXI\\1606.html}
}

@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

@online{zhangInvestigatingChainofthoughtChatGPT2024,
  title = {Investigating {{Chain-of-thought}} with {{ChatGPT}} for {{Stance Detection}} on {{Social Media}}},
  author = {Zhang, Bowen and Fu, Xianghua and Ding, Daijun and Huang, Hu and Dai, Genan and Yin, Nan and Li, Yangyang and Jing, Liwen},
  date = {2024-10-17},
  eprint = {2304.03087},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2304.03087},
  url = {http://arxiv.org/abs/2304.03087},
  urldate = {2024-11-27},
  abstract = {Stance detection predicts attitudes towards targets in texts and has gained attention with the rise of social media. Traditional approaches include conventional machine learning, early deep neural networks, and pre-trained fine-tuning models. However, with the evolution of very large pre-trained language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not requiring backpropagation training, has emerged as a promising alternative. This paper examines CoT's effectiveness in stance detection tasks, demonstrating its superior accuracy and discussing associated challenges.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\GTQYI5RZ\\Zhang et al. - 2024 - Investigating Chain-of-thought with ChatGPT for St.pdf;C\:\\Users\\griswold\\Zotero\\storage\\AD66WFSH\\2304.html}
}

@online{zhangLogicallyConsistentChainofThought2023,
  title = {A {{Logically Consistent Chain-of-Thought Approach}} for {{Stance Detection}}},
  author = {Zhang, Bowen and Ding, Daijun and Jing, Liwen and Huang, Hu},
  date = {2023-12-26},
  eprint = {2312.16054},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2312.16054},
  url = {http://arxiv.org/abs/2312.16054},
  urldate = {2024-11-27},
  abstract = {Zero-shot stance detection (ZSSD) aims to detect stances toward unseen targets. Incorporating background knowledge to enhance transferability between seen and unseen targets constitutes the primary approach of ZSSD. However, these methods often struggle with a knowledge-task disconnect and lack logical consistency in their predictions. To address these issues, we introduce a novel approach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which improves stance detection by ensuring relevant and logically sound knowledge extraction. LC-CoT employs a three-step process. Initially, it assesses whether supplementary external knowledge is necessary. Subsequently, it uses API calls to retrieve this knowledge, which can be processed by a separate LLM. Finally, a manual exemplar guides the LLM to infer stance categories, using an if-then logical structure to maintain relevance and logical coherence. This structured approach to eliciting background knowledge enhances the model's capability, outperforming traditional supervised methods without relying on labeled data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\M393VZSA\\Zhang et al. - 2023 - A Logically Consistent Chain-of-Thought Approach f.pdf;C\:\\Users\\griswold\\Zotero\\storage\\7EFDBZFJ\\2312.html}
}

@inproceedings{zhangSentimentAnalysisEra2024,
  title = {Sentiment {{Analysis}} in the {{Era}} of {{Large Language Models}}: {{A Reality Check}}},
  shorttitle = {Sentiment {{Analysis}} in the {{Era}} of {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2024},
  author = {Zhang, Wenxuan and Deng, Yue and Liu, Bing and Pan, Sinno and Bing, Lidong},
  editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
  date = {2024-06},
  pages = {3881--3906},
  publisher = {Association for Computational Linguistics},
  location = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.findings-naacl.246},
  url = {https://aclanthology.org/2024.findings-naacl.246},
  urldate = {2024-10-23},
  abstract = {Sentiment analysis (SA) has been a long-standing research area in natural language processing. With the recent advent of large language models (LLMs), there is great potential for their employment on SA problems. However, the extent to which current LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring a deeper understanding of specific sentiment phenomena or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, SentiEval, for a more comprehensive and realistic evaluation. Data and code are available at https://github.com/DAMO-NLP-SG/LLM-Sentiment.},
  eventtitle = {Findings 2024},
  file = {C:\Users\griswold\Zotero\storage\2RKASR8U\Zhang et al. - 2024 - Sentiment Analysis in the Era of Large Language Mo.pdf}
}

@online{zhangSurveyStanceDetection2024a,
  title = {A {{Survey}} of {{Stance Detection}} on {{Social Media}}: {{New Directions}} and {{Perspectives}}},
  shorttitle = {A {{Survey}} of {{Stance Detection}} on {{Social Media}}},
  author = {Zhang, Bowen and Dai, Genan and Niu, Fuqiang and Yin, Nan and Fan, Xiaomao and Huang, Hu},
  date = {2024-09-24},
  eprint = {2409.15690},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2409.15690},
  urldate = {2024-11-20},
  abstract = {In modern digital environments, users frequently express opinions on contentious topics, providing a wealth of information on prevailing attitudes. The systematic analysis of these opinions offers valuable insights for decision-making in various sectors, including marketing and politics. As a result, stance detection has emerged as a crucial subfield within affective computing, enabling the automatic detection of user stances in social media conversations and providing a nuanced understanding of public sentiment on complex issues. Recent years have seen a surge of research interest in developing effective stance detection methods, with contributions from multiple communities, including natural language processing, web science, and social computing. This paper provides a comprehensive survey of stance detection techniques on social media, covering task definitions, datasets, approaches, and future works. We review traditional stance detection models, as well as state-of-the-art methods based on large language models, and discuss their strengths and limitations. Our survey highlights the importance of stance detection in understanding public opinion and sentiment, and identifies gaps in current research. We conclude by outlining potential future directions for stance detection on social media, including the need for more robust and generalizable models, and the importance of addressing emerging challenges such as multi-modal stance detection and stance detection in low-resource languages.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\7WML7NNW\\Zhang et al. - 2024 - A Survey of Stance Detection on Social Media New .pdf;C\:\\Users\\griswold\\Zotero\\storage\\7Y9MKBJV\\2409.html}
}

@inproceedings{zhaoCalibrateUseImproving2021,
  title = {Calibrate {{Before Use}}: {{Improving Few-shot Performance}} of {{Language Models}}},
  shorttitle = {Calibrate {{Before Use}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  date = {2021-07-01},
  pages = {12697--12706},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/zhao21c.html},
  urldate = {2024-11-20},
  abstract = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model’s bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2’s accuracy (up to 30.0\% absolute) across different choices of the prompt, while also making learning considerably more stable.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\CTCF8K2J\\Zhao et al. - 2021 - Calibrate Before Use Improving Few-shot Performan.pdf;C\:\\Users\\griswold\\Zotero\\storage\\L2YDZNJI\\Zhao et al. - 2021 - Calibrate Before Use Improving Few-shot Performan.pdf}
}

@online{zhuCanChatGPTReproduce2023,
  title = {Can {{ChatGPT Reproduce Human-Generated Labels}}? {{A Study}} of {{Social Computing Tasks}}},
  shorttitle = {Can {{ChatGPT Reproduce Human-Generated Labels}}?},
  author = {Zhu, Yiming and Zhang, Peixian and Haq, Ehsan-Ul and Hui, Pan and Tyson, Gareth},
  date = {2023-04-22},
  eprint = {2304.10145},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2304.10145},
  urldate = {2024-11-20},
  abstract = {The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to relabel five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average accuracy 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9\% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploitation of ChatGPT for human annotation tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\EWPMW8YE\\Zhu et al. - 2023 - Can ChatGPT Reproduce Human-Generated Labels A St.pdf;C\:\\Users\\griswold\\Zotero\\storage\\BXYXC7TK\\2304.html}
}

@article{ziemsCanLargeLanguage2024,
  title = {Can {{Large Language Models Transform Computational Social Science}}?},
  author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  date = {2024-03-01},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {50},
  number = {1},
  pages = {237--291},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00502},
  url = {https://doi.org/10.1162/coli_a_00502},
  urldate = {2024-11-22},
  abstract = {Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\FE4E9SKU\\Ziems et al. - 2024 - Can Large Language Models Transform Computational .pdf;C\:\\Users\\griswold\\Zotero\\storage\\V792VDJU\\Can-Large-Language-Models-Transform-Computational.html}
}

@article{aldayel2021stance,
  title={Stance detection on social media: State of the art and trends},
  author={AlDayel, Abeer and Magdy, Walid},
  journal={Information Processing \& Management},
  volume={58},
  number={4},
  pages={102597},
  year={2021},
  publisher={Elsevier}
}

@misc{sheskin03,
  title={Handbook of Parametric and Nonparametric Statistical Procedures},
  author={Sheskin, D},
  year={2003},
  publisher={Chapman and Hall/CRC}
}

@article{matthews75,
  title={Comparison of the predicted and observed secondary structure of T4 phage lysozyme},
  author={Matthews, Brian W},
  journal={Biochimica et Biophysica Acta (BBA)-Protein Structure},
  volume={405},
  number={2},
  pages={442--451},
  year={1975},
  publisher={Elsevier}
}

@inbook{cramer99,
 ISBN = {9780691005478},
 URL = {http://www.jstor.org/stable/j.ctt1bpm9r4.10},
 author = {Harald Cramer},
 booktitle = {{M}athematical {M}ethods of {S}tatistics},
 pages = {260--320},
 publisher = {Princeton University Press},
 %title = {Variables and Distributions in $\mathbb{R}_n$},
 title = {Chapter 21: {T}he {T}wo-{D}imensional {C}ase},
 urldate = {2025-01-20},
 year = {1999}
}
