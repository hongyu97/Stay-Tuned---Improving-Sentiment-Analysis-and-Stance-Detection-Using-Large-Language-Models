@article{alonsoSentimentAnalysisFake2021,
  title = {Sentiment {{Analysis}} for {{Fake News Detection}}},
  author = {Alonso, Miguel A. and Vilares, David and {G{\'o}mez-Rodr{\'i}guez}, Carlos and Vilares, Jes{\'u}s},
  year = {2021},
  month = jan,
  journal = {Electronics},
  volume = {10},
  number = {11},
  pages = {1348},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics10111348},
  urldate = {2023-12-31},
  abstract = {In recent years, we have witnessed a rise in fake news, i.e., provably false pieces of information created with the intention of deception. The dissemination of this type of news poses a serious threat to cohesion and social well-being, since it fosters political polarization and the distrust of people with respect to their leaders. The huge amount of news that is disseminated through social media makes manual verification unfeasible, which has promoted the design and implementation of automatic systems for fake news detection. The creators of fake news use various stylistic tricks to promote the success of their creations, with one of them being to excite the sentiments of the recipients. This has led to sentiment analysis, the part of text analytics in charge of determining the polarity and strength of sentiments expressed in a text, to be used in fake news detection approaches, either as a basis of the system or as a complementary element. In this article, we study the different uses of sentiment analysis in the detection of fake news, with a discussion of the most relevant elements and shortcomings, and the requirements that should be met in the near future, such as multilingualism, explainability, mitigation of biases, or treatment of multimedia elements.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {fake news,opinion mining,sentiment analysis,social media},
  file = {C:\Users\griswold\Zotero\storage\A83LMEQP\Alonso et al. - 2021 - Sentiment Analysis for Fake News Detection.pdf}
}

@inproceedings{baccianellaSENTIWORDNETEnhancedLexical,
  title={Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining.},
  author={Baccianella, Stefano and Esuli, Andrea and Sebastiani, Fabrizio and others},
  booktitle={Lrec},
  volume={10},
  number={2010},
  pages={2200--2204},
  year={2010},
  organization={Valletta}
}

@article{beigiOverviewSentimentAnalysis2016,
  title = {An {{Overview}} of {{Sentiment Analysis}} in {{Social Media}} and {{Its Applications}} in {{Disaster Relief}}},
  author = {Beigi, Ghazaleh and Hu, Xia and Maciejewski, Ross and Liu, Huan},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  year = {2016},
  journal = {Sentiment Analysis and Ontology Engineering: An Environment of Computational Intelligence},
  series = {Studies in {{Computational Intelligence}}},
  pages = {313--340},
  doi = {10.1007/978-3-319-30319-2_13},
  urldate = {2023-12-31},
  abstract = {Sentiment analysis refers to the class of computational and natural language processingNatural language processingbasedSentiment analysistechniques used to identify, extract or characterize subjective information, such as opinions, expressed in a given piece of text. The main purpose of sentiment analysis is to classify a writer's attitude towards various topics into positive, negative or neutral categories. Sentiment analysis has many applications in different domains including, but not limited to, business intelligence, politics, sociology, etc. Recent years, on the other hand, have witnessed the advent of social networking websites, microblogs, wikis and Web applications and consequently, an unprecedented growth in user-generated data is poised for sentiment mining. Data such as web-postings, Tweets, videos, etc., all express opinions on various topics and events, offer immense opportunities to study and analyze human opinions and sentiment. In this chapter, we study the information published by individuals in social media in cases of natural disasters and emergencies and investigate if such information could be used by first responders to improve situational awareness and crisis management. In particular, we explore applications of sentiment analysisSentiment analysisand demonstrate how sentiment mining in social mediaSocial mediacan be exploited to determine how local crowds react during a disaster, and how such information can be used to improve disaster management. Such information can also be used to help assess the extent of the devastation and find people who are in specific need during an emergency situation. We first provide the formal definition of sentiment analysis in social media and cover traditional and the state-of-the-art approaches while highlighting contributions, shortcomings, and pitfalls due to the composition of online media streams. Next we discuss the relationship among social media, disaster reliefDisaster reliefand situational awareness and explain how social media is used in these contexts with the focus on sentiment analysis. In order to enable quick analysis of real-time geo-distributed data, we will detail applications of visual analytics with an emphasis on sentiment visualizationVisualization. Finally, we conclude the chapter with a discussion of research challenges in sentiment analysis and its application in disaster relief.},
  langid = {english},
  keywords = {Disaster relief,Sentiment analysis,Social media,Visualization}
}

@article{bestvaterSentimentNotStance2023,
  title = {Sentiment Is {{Not Stance}}: {{Target-Aware Opinion Classification}} for {{Political Text Analysis}}},
  shorttitle = {Sentiment Is {{Not Stance}}},
  author = {Bestvater, Samuel E. and Monroe, Burt L.},
  year = {2023},
  month = apr,
  journal = {Political Analysis},
  volume = {31},
  number = {2},
  pages = {235--256},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2022.10},
  urldate = {2023-09-05},
  abstract = {Sentiment analysis techniques have a long history in natural language processing and have become a standard tool in the analysis of political texts, promising a conceptually straightforward automated method of extracting meaning from textual data by scoring documents on a scale from positive to negative. However, while these kinds of sentiment scores can capture the overall tone of a document, the underlying concept of interest for political analysis is often actually the document's stance with respect to a given target---how positively or negatively it frames a specific idea, individual, or group---as this reflects the author's underlying political attitudes. In this paper, we question the validity of approximating author stance through sentiment scoring in the analysis of political texts, and advocate for greater attention to be paid to the conceptual distinction between a document's sentiment and its stance. Using examples from open-ended survey responses and from political discussions on social media, we demonstrate that in many political text analysis applications, sentiment and stance do not necessarily align, and therefore sentiment analysis methods fail to reliably capture ground-truth document stance, amplifying noise in the data and leading to faulty conclusions.},
  langid = {english},
  keywords = {machine learning,political stance,sentiment analysis,text-as-data},
  file = {C:\Users\griswold\Zotero\storage\EEFSY2GC\Bestvater and Monroe - 2023 - Sentiment is Not Stance Target-Aware Opinion Clas.pdf}
}

@article{boukesWhatToneEasy2020,
  title = {What's the {{Tone}}? {{Easy Doesn}}'t {{Do It}}: {{Analyzing Performance}} and {{Agreement Between Off-the-Shelf Sentiment Analysis Tools}}},
  shorttitle = {What's the {{Tone}}?},
  author = {Boukes, Mark and {van de Velde}, Bob and Araujo, Theo and Vliegenthart, Rens},
  year = {2020},
  month = apr,
  journal = {Communication Methods and Measures},
  volume = {14},
  number = {2},
  pages = {83--104},
  publisher = {Routledge},
  issn = {1931-2458},
  doi = {10.1080/19312458.2019.1671966},
  urldate = {2024-01-02},
  abstract = {This article scrutinizes the method of automated content analysis to measure the tone of news coverage. We compare a range of off-the-shelf sentiment analysis tools to manually coded economic news as well as examine the agreement between these dictionary approaches themselves. We assess the performance of five off-the-shelf sentiment analysis tools and two tailor-made dictionary-based approaches. The analyses result in five conclusions. First, there is little overlap between the off-the-shelf tools; causing wide divergence in terms of tone measurement. Second, there is no stronger overlap with manual coding for short texts (i.e., headlines) than for long texts (i.e., full articles). Third, an approach that combines individual dictionaries achieves a comparably good performance.~Fourth, precision may increase to acceptable levels at higher levels of granularity. Fifth, performance of dictionary approaches depends more on the number of relevant keywords in the dictionary than on the number of valenced words as such; a small tailor-made lexicon was not inferior to large established dictionaries. Altogether, we conclude that off-the-shelf sentiment analysis tools are mostly unreliable and unsuitable for research purposes -- at least in the context of Dutch economic news -- and manual validation for the specific language, domain, and genre of the research project at hand is always warranted.},
  file = {C:\Users\griswold\Zotero\storage\W7PLTDGR\Boukes et al. - 2020 - What’s the Tone Easy Doesn’t Do It Analyzing Per.pdf}
}

@article{cambriaSenticNetSemanticResource,
  title = {{{SenticNet}} 4: {{A Semantic Resource}} for {{Sentiment Analysis Based}} on {{Conceptual Primitives}}},
  author = {Cambria, Erik and Poria, Soujanya and Bajpai, Rajiv and Schuller, Bjoern},
  abstract = {An important difference between traditional AI systems and human intelligence is the human ability to harness commonsense knowledge gleaned from a lifetime of learning and experience to make informed decisions. This allows humans to adapt easily to novel situations where AI fails catastrophically due to a lack of situation-specific rules and generalization capabilities. Commonsense knowledge also provides background information that enables humans to successfully operate in social situations where such knowledge is typically assumed. Since commonsense consists of information that humans take for granted, gathering it is an extremely difficult task. Previous versions of SenticNet were focused on collecting this kind of knowledge for sentiment analysis but they were heavily limited by their inability to generalize. SenticNet 4 overcomes such limitations by leveraging on conceptual primitives automatically generated by means of hierarchical clustering and dimensionality reduction.},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\C87I5T9S\Cambria et al. - SenticNet 4 A Semantic Resource for Sentiment Ana.pdf}
}

@article{ceronEveryTweetCounts2014,
  title = {Every Tweet Counts? {{How}} Sentiment Analysis of Social Media Can Improve Our Knowledge of Citizens' Political Preferences with an Application to {{Italy}} and {{France}}},
  shorttitle = {Every Tweet Counts?},
  author = {Ceron, Andrea and Curini, Luigi and Iacus, Stefano M and Porro, Giuseppe},
  year = {2014},
  month = mar,
  journal = {New Media \& Society},
  volume = {16},
  number = {2},
  pages = {340--358},
  publisher = {SAGE Publications},
  issn = {1461-4448},
  doi = {10.1177/1461444813480466},
  urldate = {2024-01-02},
  abstract = {The growing usage of social media by a wider audience of citizens sharply increases the possibility of investigating the web as a device to explore and track political preferences. In the present paper we apply a method recently proposed by other social scientists to three different scenarios, by analyzing on one side the online popularity of Italian political leaders throughout 2011, and on the other the voting intention of French Internet users in both the 2012 presidential ballot and the subsequent legislative election. While Internet users are not necessarily representative of the whole population of a country's citizens, our analysis shows a remarkable ability for social media to forecast electoral results, as well as a noteworthy correlation between social media and the results of traditional mass surveys. We also illustrate that the predictive ability of social media analysis strengthens as the number of citizens expressing their opinion online increases, provided that the citizens act consistently on these opinions.},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\UXAZ2GAC\Ceron et al. - 2014 - Every tweet counts How sentiment analysis of soci.pdf}
}

@article{chauhanEmergenceSocialMedia2021,
  title = {The Emergence of Social Media Data and Sentiment Analysis in Election Prediction},
  author = {Chauhan, Priyavrat and Sharma, Nonita and Sikka, Geeta},
  year = {2021},
  month = feb,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  volume = {12},
  number = {2},
  pages = {2601--2627},
  issn = {1868-5145},
  doi = {10.1007/s12652-020-02423-y},
  urldate = {2023-09-05},
  abstract = {This work presents and assesses the power of various volumetric, sentiment, and social network approaches to predict crucial decisions from online social media platforms. The views of individuals play a vital role in the discovery of some critical decisions. Social media has become a well-known platform for voicing the feelings of the general population around the globe for almost decades. Sentiment analysis or opinion mining is a method that is used to mine the general population's views or feelings. In this respect, the forecasting of election results is an application of sentiment analysis aimed at predicting the outcomes of an ongoing election by gauging the mood of the public through social media. This survey paper outlines the evaluation of sentiment analysis techniques and tries to edify the contribution of the researchers to predict election results through social media content. This paper also gives a review of studies that tried to infer the political stance of online users using social media platforms such as Facebook and Twitter. Besides, this paper highlights the research challenges associated with predicting election results and open issues related to sentiment analysis. Further, this paper also suggests some future directions in respective election prediction using social media content.},
  langid = {english},
  keywords = {Election prediction,Opinion mining,Sentiment analysis,Social media,Twitter},
  file = {C:\Users\griswold\Zotero\storage\855M3BUV\Chauhan et al. - 2021 - The emergence of social media data and sentiment a.pdf}
}

@inproceedings{dengMPQAEntityEventLevel2015,
  title = {{{MPQA}} 3.0: {{An Entity}}/{{Event-Level Sentiment Corpus}}},
  shorttitle = {{{MPQA}} 3.0},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Deng, Lingjia and Wiebe, Janyce},
  editor = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
  year = {2015},
  month = may,
  pages = {1323--1328},
  publisher = {Association for Computational Linguistics},
  address = {Denver, Colorado},
  doi = {10.3115/v1/N15-1146},
  urldate = {2024-01-02},
  file = {C:\Users\griswold\Zotero\storage\7DCRXAGG\Deng and Wiebe - 2015 - MPQA 3.0 An EntityEvent-Level Sentiment Corpus.pdf}
}

@misc{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2024-01-12},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\MBL9EXJM\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;C\:\\Users\\griswold\\Zotero\\storage\\4S766QI5\\1810.html}
}

@article{drusSentimentAnalysisSocial2019,
  title = {Sentiment {{Analysis}} in {{Social Media}} and {{Its Application}}: {{Systematic Literature Review}}},
  shorttitle = {Sentiment {{Analysis}} in {{Social Media}} and {{Its Application}}},
  author = {Drus, Zulfadzli and Khalid, Haliyana},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The {{Fifth Information Systems International Conference}}, 23-24 {{July}} 2019, {{Surabaya}}, {{Indonesia}}},
  volume = {161},
  pages = {707--714},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.11.174},
  urldate = {2023-09-05},
  abstract = {This paper is a report of a review on sentiment analysis in social media that explored the methods, social media platform used and its application. Social media contain a large amount of raw data that has been uploaded by users in the form of text, videos, photos and audio. The data can be converted into valuable information by using sentiment analysis. A systematic review of studies published between 2014 to 2019 was undertaken using the following trusted and credible database including ACM, Emerald Insight, IEEE Xplore, Science Direct and Scopus. After the initial and in-depth screening of paper, 24 out of 77 articles have been chosen from the review process. The articles have been reviewed based on the aim of the study. The result shows most of the articles applied opinion-lexicon method to analyses text sentiment in social media, extracted data on microblogging site mainly Twitter and sentiment analysis application can be seen in world events, healthcare, politics and business.},
  keywords = {Big data,Sentiment analysis,Social media},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\PBBZMNJ3\\Drus and Khalid - 2019 - Sentiment Analysis in Social Media and Its Applica.pdf;C\:\\Users\\griswold\\Zotero\\storage\\CDKMZ2H7\\S187705091931885X.html}
}

@article{du2007stance,
  title = {The {{Stance Triangle}}},
  author = {Du Bois, John W},
  year = {2007},
  journal = {Stancetaking in discourse: Subjectivity, evaluation, interaction},
  volume = {164},
  number = {3},
  pages = {139--182}
}

@article{greavesUseSentimentAnalysis2013,
  title = {Use of {{Sentiment Analysis}} for {{Capturing Patient Experience From Free-Text Comments Posted Online}}},
  author = {Greaves, Felix and {Ramirez-Cano}, Daniel and Millett, Christopher and Darzi, Ara and Donaldson, Liam},
  year = {2013},
  month = nov,
  journal = {Journal of Medical Internet Research},
  volume = {15},
  number = {11},
  pages = {e2721},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/jmir.2721},
  urldate = {2023-12-31},
  abstract = {Background: There are large amounts of unstructured, free-text information about quality of health care available on the Internet in blogs, social networks, and on physician rating websites that are not captured in a systematic way. New analytical techniques, such as sentiment analysis, may allow us to understand and use this information more effectively to improve the quality of health care. Objective: We attempted to use machine learning to understand patients\&\#8217; unstructured comments about their care. We used sentiment analysis techniques to categorize online free-text comments by patients as either positive or negative descriptions of their health care. We tried to automatically predict whether a patient would recommend a hospital, whether the hospital was clean, and whether they were treated with dignity from their free-text description, compared to the patient\&\#8217;s own quantitative rating of their care. Methods: We applied machine learning techniques to all 6412 online comments about hospitals on the English National Health Service website in 2010 using Weka data-mining software. We also compared the results obtained from sentiment analysis with the paper-based national inpatient survey results at the hospital level using Spearman rank correlation for all 161 acute adult hospital trusts in England. Results: There was 81\%, 84\%, and 89\% agreement between quantitative ratings of care and those derived from free-text comments using sentiment analysis for cleanliness, being treated with dignity, and overall recommendation of hospital respectively (kappa scores: .40\&\#8211;.74, P\&\#60;.001 for all). We observed mild to moderate associations between our machine learning predictions and responses to the large patient survey for the three categories examined (Spearman rho 0.37-0.51, P\&\#60;.001 for all). Conclusions: The prediction accuracy that we have achieved using this machine learning process suggests that we are able to predict, from free-text, a reasonably accurate assessment of patients\&\#8217; opinion about different performance aspects of a hospital and that these machine learning predictions are associated with results of more conventional surveys.},
  langid = {english},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\2XFAH7QJ\\Greaves et al. - 2013 - Use of Sentiment Analysis for Capturing Patient Ex.pdf;C\:\\Users\\griswold\\Zotero\\storage\\VT7RKAQW\\e239.html}
}

@misc{hardalovSurveyStanceDetection2022a,
  title = {A {{Survey}} on {{Stance Detection}} for {{Mis-}} and {{Disinformation Identification}}},
  author = {Hardalov, Momchil and Arora, Arnav and Nakov, Preslav and Augenstein, Isabelle},
  year = {2022},
  month = may,
  number = {arXiv:2103.00242},
  eprint = {2103.00242},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00242},
  urldate = {2024-01-12},
  abstract = {Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\D97E9M8J\\Hardalov et al. - 2022 - A Survey on Stance Detection for Mis- and Disinfor.pdf;C\:\\Users\\griswold\\Zotero\\storage\\8SKZW4I8\\2103.html}
}

@article{hartmannMoreFeelingAccuracy2023,
  title = {More than a {{Feeling}}: {{Accuracy}} and {{Application}} of {{Sentiment Analysis}}},
  shorttitle = {More than a {{Feeling}}},
  author = {Hartmann, Jochen and Heitmann, Mark and Siebert, Christian and Schamp, Christina},
  year = {2023},
  month = mar,
  journal = {International Journal of Research in Marketing},
  volume = {40},
  number = {1},
  pages = {75--87},
  issn = {0167-8116},
  doi = {10.1016/j.ijresmar.2022.05.005},
  urldate = {2024-01-12},
  abstract = {Sentiment is fundamental to human communication. Countless marketing applications mine opinions from social media communication, news articles, customer feedback, or corporate communication. Various sentiment analysis methods are available and new ones have recently been proposed. Lexicons can relate individual words and expressions to sentiment scores. In contrast, machine learning methods are more complex to interpret, but promise higher accuracy, i.e., fewer false classifications. We propose an empirical framework and quantify these trade-offs for different types of research questions, data characteristics, and analytical resources to enable informed method decisions contingent on the application context. Based on a meta-analysis of 272 datasets and 12 million sentiment-labeled text documents, we find that the recently proposed transfer learning models indeed perform best, but can perform worse than popular leaderboard benchmarks suggest. We quantify the accuracy-interpretability trade-off, showing that, compared to widely established lexicons, transfer learning models on average classify more than 20 percentage points more documents correctly. To form realistic performance expectations, additional context variables, most importantly the desired number of sentiment classes and the text length, should be taken into account. We provide a pre-trained sentiment analysis model (called SiEBERT) with open-source scripts that can be applied as easily as an off-the-shelf lexicon.},
  keywords = {Deep Contextual Language Models,Machine Learning,Meta-Analysis,Natural Language Processing,Sentiment Analysis,Text Mining,Transfer Learning},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\T6CTZKIQ\\Hartmann et al. - 2023 - More than a Feeling Accuracy and Application of S.pdf;C\:\\Users\\griswold\\Zotero\\storage\\UUHWJHX6\\S0167811622000477.html}
}

@inproceedings{huMiningSummarizingCustomer2004a,
  title = {Mining and Summarizing Customer Reviews},
  booktitle = {Proceedings of the Tenth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Hu, Minqing and Liu, Bing},
  year = {2004},
  month = aug,
  pages = {168--177},
  publisher = {ACM},
  address = {Seattle WA USA},
  doi = {10.1145/1014052.1014073},
  urldate = {2024-01-02},
  isbn = {978-1-58113-888-7},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\X9F6LC54\Hu and Liu - 2004 - Mining and summarizing customer reviews.pdf}
}

@article{huttoVADERParsimoniousRuleBased2014,
  title = {{{VADER}}: {{A Parsimonious Rule-Based Model}} for {{Sentiment Analysis}} of {{Social Media Text}}},
  shorttitle = {{{VADER}}},
  author = {Hutto, C. and Gilbert, Eric},
  year = {2014},
  month = may,
  journal = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume = {8},
  number = {1},
  pages = {216--225},
  issn = {2334-0770},
  doi = {10.1609/icwsm.v8i1.14550},
  urldate = {2024-01-02},
  abstract = {The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a gold-standard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks.},
  copyright = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
  langid = {english},
  keywords = {Human Centered Computing},
  file = {C:\Users\griswold\Zotero\storage\DVBYYCM7\Hutto and Gilbert - 2014 - VADER A Parsimonious Rule-Based Model for Sentime.pdf}
}

@article{kazmaierPowerEnsembleLearning2022,
  title = {The Power of Ensemble Learning in Sentiment Analysis},
  author = {Kazmaier, Jacqueline and {van Vuuren}, Jan H.},
  year = {2022},
  month = jan,
  journal = {Expert Systems with Applications},
  volume = {187},
  pages = {115819},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2021.115819},
  urldate = {2023-09-05},
  abstract = {An ensemble of models is a set of learning models whose individual predictions are combined in such a way that component models compensate for each other's weaknesses. Although there has been a growing interest in ensemble learning techniques in the general machine learning community, the use of ensembles in sentiment classification is still limited. Moreover, much of the research activity on ensemble learning is centred around homogeneous ensembles, although heterogeneous ensembles may prove very useful when combining pre-trained models, which are often readily available. In this paper, several techniques for constructing heterogeneous ensembles are applied and comparatively evaluated in respect of benchmark sentiment classification data sets across four different domains. Median performance improvements of up to 5.53\% over the best individual model are observed for several ensemble configurations in respect of all four validation data sets, and clear trends are identified that may prove useful to other researchers in the field. Furthermore, a novel ensemble selection approach is proposed that avoids the storage of individual predictions, as well as the costly retraining of all candidate models for an ensemble, that are often required by other similar approaches.},
  keywords = {Ensemble learning,Machine learning,Natural language processing,Sentiment analysis},
  file = {C:\Users\griswold\Zotero\storage\WJU7YBF3\Kazmaier and van Vuuren - 2022 - The power of ensemble learning in sentiment analys.pdf}
}

@inproceedings{kumawatSentimentAnalysisUsing2021,
  title = {Sentiment {{Analysis Using Language Models}}: {{A Study}}},
  shorttitle = {Sentiment {{Analysis Using Language Models}}},
  booktitle = {2021 11th {{International Conference}} on {{Cloud Computing}}, {{Data Science}} \& {{Engineering}} ({{Confluence}})},
  author = {Kumawat, Spraha and Yadav, Inna and Pahal, Nisha and Goel, Deepti},
  year = {2021},
  month = jan,
  pages = {984--988},
  doi = {10.1109/Confluence51648.2021.9377043},
  urldate = {2024-05-13},
  abstract = {Sentiment analysis is concerned with extracting sentiment to ascertain the attitudes, and emotions associated with the text. It is broadly applied to voice of the customer as they convey their experience and feelings more blatantly which is why comprehending customer's emotions is a must. To identify and classify these unstructured emotions natural language processing (NLP) and machine learning approaches have been adopted in recent times. The main issue with the existing techniques is the inability to deal with the correct interpretation of context owing to lack of labeled data. In this paper, we studied deep neural network based language models to interpret and classify textual sequences into positive, negative or neutral emotions which remove the bottleneck of explicit human labeling. These models were analyzed and evaluations were performed on the Twitter US Airline Sentiment dataset. We have observed a considerable amount of improvements with respect to prior state-of-the-art approaches which closes the gap with supervised feature learning.},
  keywords = {Analytical models,Atmospheric modeling,Data models,Deep learning,Deep Learning,Language Models,Sentiment analysis,Sentiment Analysis,Social networking (online),Transformer cores,Transformers},
  file = {C:\Users\griswold\Zotero\storage\MBEDUR3G\9377043.html}
}

@article{baldi2000assessing,
  title={Assessing the accuracy of prediction algorithms for classification: an overview},
  author={Baldi, Pierre and Brunak, S{\o}ren and Chauvin, Yves and Andersen, Claus AF and Nielsen, Henrik},
  journal={Bioinformatics},
  volume={16},
  number={5},
  pages={412--424},
  year={2000},
  publisher={Oxford University Press}
}

@article{chicco2020advantages,
  title={The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
  author={Chicco, Davide and Jurman, Giuseppe},
  journal={BMC genomics},
  volume={21},
  pages={1--13},
  year={2020},
  publisher={Springer}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zhang2015cross,
  title={Cross-validation for selecting a model selection procedure},
  author={Zhang, Yongli and Yang, Yuhong},
  journal={Journal of Econometrics},
  volume={187},
  number={1},
  pages={95--112},
  year={2015},
  publisher={Elsevier}
}

@article{saleena2018ensemble,
  title={An ensemble classification system for twitter sentiment analysis},
  author={Saleena, Nabizath and others},
  journal={Procedia computer science},
  volume={132},
  pages={937--946},
  year={2018},
  publisher={Elsevier}
}

@article{chan2023state,
  title={State of the art: a review of sentiment analysis based on sequential transfer learning},
  author={Chan, Jireh Yi-Le and Bea, Khean Thye and Leow, Steven Mun Hong and Phoong, Seuk Wai and Cheng, Wai Khuen},
  journal={Artificial Intelligence Review},
  volume={56},
  number={1},
  pages={749--780},
  year={2023},
  publisher={Springer}
}

@article{barbieri2020tweeteval,
  title={Tweeteval: Unified benchmark and comparative evaluation for tweet classification},
  author={Barbieri, Francesco and Camacho-Collados, Jose and Neves, Leonardo and Espinosa-Anke, Luis},
  journal={arXiv preprint arXiv:2010.12421},
  year={2020}
}

@article{deng2022rlprompt,
  title={Rlprompt: Optimizing discrete text prompts with reinforcement learning},
  author={Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric P and Hu, Zhiting},
  journal={arXiv preprint arXiv:2205.12548},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{loureiro2022timelmsdiachroniclanguagemodels,
      title={TimeLMs: Diachronic Language Models from Twitter}, 
      author={Daniel Loureiro and Francesco Barbieri and Leonardo Neves and Luis Espinosa Anke and Jose Camacho-Collados},
      year={2022},
      eprint={2202.03829},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.03829}, 
}

@misc{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.11692},
  urldate = {2024-01-12},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\EEGKPKFA\\Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;C\:\\Users\\griswold\\Zotero\\storage\\RXHLPEKG\\1907.html}
}

@inproceedings{mohammadEmotionsEvokedCommon2010,
  title = {Emotions {{Evoked}} by {{Common Words}} and {{Phrases}}: {{Using Mechanical Turk}} to {{Create}} an {{Emotion Lexicon}}},
  shorttitle = {Emotions {{Evoked}} by {{Common Words}} and {{Phrases}}},
  booktitle = {Proceedings of the {{NAACL HLT}} 2010 {{Workshop}} on {{Computational Approaches}} to {{Analysis}} and {{Generation}} of {{Emotion}} in {{Text}}},
  author = {Mohammad, Saif and Turney, Peter},
  editor = {Inkpen, Diana and Strapparava, Carlo},
  year = {2010},
  month = jun,
  pages = {26--34},
  publisher = {Association for Computational Linguistics},
  address = {Los Angeles, CA},
  urldate = {2024-01-02},
  file = {C:\Users\griswold\Zotero\storage\CLSY2P73\Mohammad and Turney - 2010 - Emotions Evoked by Common Words and Phrases Using.pdf}
}

@inproceedings{mohammadSemEval2016TaskDetecting2016,
  title = {{{SemEval-2016 Task}} 6: {{Detecting Stance}} in {{Tweets}}},
  shorttitle = {{{SemEval-2016 Task}} 6},
  booktitle = {Proceedings of the 10th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval-2016}})},
  author = {Mohammad, Saif and Kiritchenko, Svetlana and Sobhani, Parinaz and Zhu, Xiaodan and Cherry, Colin},
  editor = {Bethard, Steven and Carpuat, Marine and Cer, Daniel and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
  year = {2016},
  month = jun,
  pages = {31--41},
  publisher = {Association for Computational Linguistics},
  address = {San Diego, California},
  doi = {10.18653/v1/S16-1003},
  urldate = {2023-12-31},
  file = {C:\Users\griswold\Zotero\storage\FT4WL5E9\Mohammad et al. - 2016 - SemEval-2016 Task 6 Detecting Stance in Tweets.pdf}
}

@inproceedings{nasukawaSentimentAnalysisCapturing2003,
  title = {Sentiment Analysis: Capturing Favorability Using Natural Language Processing},
  shorttitle = {Sentiment Analysis},
  booktitle = {Proceedings of the 2nd International Conference on {{Knowledge}} Capture},
  author = {Nasukawa, Tetsuya and Yi, Jeonghee},
  year = {2003},
  month = oct,
  series = {K-{{CAP}} '03},
  pages = {70--77},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/945645.945658},
  urldate = {2024-01-02},
  abstract = {This paper illustrates a sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from a document, instead of classifying the whole document into positive or negative.The essential issues in sentiment analysis are to identify how sentiments are expressed in texts and whether the expressions indicate positive (favorable) or negative (unfavorable) opinions toward the subject. In order to improve the accuracy of the sentiment analysis, it is important to properly identify the semantic relationships between the sentiment expressions and the subject. By applying semantic analysis with a syntactic parser and sentiment lexicon, our prototype system achieved high precision (75-95\%, depending on the data) in finding sentiments within Web pages and news articles.},
  isbn = {978-1-58113-583-1},
  keywords = {favorability analysis,information extraction,sentiment analysis,text mining},
  file = {C:\Users\griswold\Zotero\storage\I6PZKKAW\Nasukawa and Yi - 2003 - Sentiment analysis capturing favorability using n.pdf}
}

@misc{openaiIntroducingChatGPT,
  title = {Introducing {{ChatGPT}}},
  author = {OpenAI},
  urldate = {2024-01-12},
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  howpublished = {https://openai.com/blog/chatgpt},
  langid = {american},
  file = {C:\Users\griswold\Zotero\storage\3MMA4FHP\chatgpt.html}
}

@misc{pangThumbsSentimentClassification2002,
  title = {Thumbs up? {{Sentiment Classification}} Using {{Machine Learning Techniques}}},
  shorttitle = {Thumbs Up?},
  author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  year = {2002},
  month = may,
  number = {arXiv:cs/0205070},
  eprint = {cs/0205070},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cs/0205070},
  urldate = {2024-01-02},
  abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.6,I.2.7},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\SDAB3ILM\\Pang et al. - 2002 - Thumbs up Sentiment Classification using Machine .pdf;C\:\\Users\\griswold\\Zotero\\storage\\IPX6UJXR\\0205070.html}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\66AICYYZ\Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@inproceedings{socherRecursiveDeepModels2013,
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality Over}} a {{Sentiment Treebank}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
  editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
  year = {2013},
  month = oct,
  pages = {1631--1642},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, Washington, USA},
  urldate = {2024-01-12},
  file = {C:\Users\griswold\Zotero\storage\Y86CSNZV\Socher et al. - 2013 - Recursive Deep Models for Semantic Compositionalit.pdf}
}

@article{suAnalyzingPublicSentiments2017,
  title = {Analyzing Public Sentiments Online: Combining Human- and Computer-Based Content Analysis},
  shorttitle = {Analyzing Public Sentiments Online},
  author = {Su, Leona Yi-Fan and Cacciatore, Michael A. and Liang, Xuan and Brossard, Dominique and Scheufele, Dietram A. and Xenos, Michael A.},
  year = {2017},
  month = mar,
  journal = {Information, Communication \& Society},
  volume = {20},
  number = {3},
  pages = {406--427},
  publisher = {Routledge},
  issn = {1369-118X},
  doi = {10.1080/1369118X.2016.1182197},
  urldate = {2023-09-05},
  abstract = {Recent technological developments have created novel opportunities for analyzing and identifying patterns in large volumes of digital content. However, many content analysis tools require researchers to choose between the validity of human-based coding and the ability to analyze large volumes of content through computer-based techniques. This study argues for the use of supervised content analysis tools that capitalize on the strengths of human- and computer-based coding for assessing opinion expression. We begin by outlining the key methodological issues surrounding content analysis as performed by human coders and existing computational algorithms. After reviewing the most popular analytic approaches, we introduce an alternative, hybrid method that is aimed at improving reliability, validity, and efficiency when analyzing social media content. To demonstrate the usefulness of this method, we track nuclear energy- and nanotechnology-related opinion expression on Twitter surrounding the Fukushima Daiichi accident to examine the extent to which the volume and tone of tweets shift in directions consistent with the expected external influence of the event. Our analysis revealed substantial shifts in both the volume and tone of nuclear power-related tweets that were consistent with our expectations following the disaster event. Conversely, there was decidedly more stability in the volume and tone of tweets for our comparison issue. These analyses provide an empirical demonstration of how the presented hybrid method can analyze defined communication sentiment and topics from large-scale social media data sets. The implications for communication scholars are discussed.},
  keywords = {computer-based coding,Content analysis,human-based coding,sentiment analysis,supervised machine learning,Twitter},
  file = {C:\Users\griswold\Zotero\storage\E5ZQ75X5\Su et al. - 2017 - Analyzing public sentiments online combining huma.pdf}
}

@article{taboadaLexiconBasedMethodsSentiment2011,
  title = {Lexicon-{{Based Methods}} for {{Sentiment Analysis}}},
  author = {Taboada, Maite and Brooke, Julian and Tofiloski, Milan and Voll, Kimberly and Stede, Manfred},
  year = {2011},
  month = jun,
  journal = {Computational Linguistics},
  volume = {37},
  number = {2},
  pages = {267--307},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/COLI_a_00049},
  urldate = {2024-01-02},
  abstract = {We present a lexicon-based approach to extracting sentiment from text. The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensification and negation. SO-CAL is applied to the polarity classification task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and in completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.},
  langid = {english},
  file = {C:\Users\griswold\Zotero\storage\LAMZ77TX\Taboada et al. - 2011 - Lexicon-Based Methods for Sentiment Analysis.pdf}
}

@misc{wangLargeLanguageModels2023,
  title = {Large {{Language Models Are Zero-Shot Text Classifiers}}},
  author = {Wang, Zhiqiang and Pang, Yiran and Lin, Yanbin},
  year = {2023},
  month = dec,
  number = {arXiv:2312.01044},
  eprint = {2312.01044},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.01044},
  urldate = {2023-12-31},
  abstract = {Retrained large language models (LLMs) have become extensively used across various sub-disciplines of natural language processing (NLP). In NLP, text classification problems have garnered considerable focus, but still faced with some limitations related to expensive computational cost, time consumption, and robust performance to unseen classes. With the proposal of chain of thought prompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with the step by step reasoning prompts, instead of conventional question and answer formats. The zero-shot LLMs in the text classification problems can alleviate these limitations by directly utilizing pretrained models to predict both seen and unseen classes. Our research primarily validates the capability of GPT models in text classification. We focus on effectively utilizing prompt strategies to various text classification scenarios. Besides, we compare the performance of zero shot LLMs with other state of the art text classification methods, including traditional machine learning methods, deep learning methods, and ZSL methods. Experimental results demonstrate that the performance of LLMs underscores their effectiveness as zero-shot text classifiers in three of the four datasets analyzed. The proficiency is especially advantageous for small businesses or teams that may not have extensive knowledge in text classification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\griswold\\Zotero\\storage\\C82IVZE3\\Wang et al. - 2023 - Large Language Models Are Zero-Shot Text Classifie.pdf;C\:\\Users\\griswold\\Zotero\\storage\\GSNAZZ28\\2312.html}
}

@article{wangSurveyZeroShotLearning2019,
  title = {A {{Survey}} of {{Zero-Shot Learning}}: {{Settings}}, {{Methods}}, and {{Applications}}},
  shorttitle = {A {{Survey}} of {{Zero-Shot Learning}}},
  author = {Wang, Wei and Zheng, Vincent W. and Yu, Han and Miao, Chunyan},
  year = {2019},
  month = jan,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {10},
  number = {2},
  pages = {13:1--13:37},
  issn = {2157-6904},
  doi = {10.1145/3293318},
  urldate = {2023-12-31},
  abstract = {Most machine-learning methods focus on classifying instances whose classes have already been seen in training. In practice, many applications require classifying instances whose classes have not been seen previously. Zero-shot learning is a powerful and promising learning paradigm, in which the classes covered by training instances and the classes we aim to classify are disjoint. In this paper, we provide a comprehensive survey of zero-shot learning. First of all, we provide an overview of zero-shot learning. According to the data utilized in model optimization, we classify zero-shot learning into three learning settings. Second, we describe different semantic spaces adopted in existing zero-shot learning works. Third, we categorize existing zero-shot learning methods and introduce representative methods under each category. Fourth, we discuss different applications of zero-shot learning. Finally, we highlight promising future research directions of zero-shot learning.},
  keywords = {Zero-shot learning survey},
  file = {C:\Users\griswold\Zotero\storage\RQ4AA89H\Wang et al. - 2019 - A Survey of Zero-Shot Learning Settings, Methods,.pdf}
}

@article{wankhadeSurveySentimentAnalysis2022,
  title = {A Survey on Sentiment Analysis Methods, Applications, and Challenges},
  author = {Wankhade, Mayur and Rao, Annavarapu Chandra Sekhara and Kulkarni, Chaitanya},
  year = {2022},
  month = oct,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {7},
  pages = {5731--5780},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10144-1},
  urldate = {2024-05-13},
  abstract = {The rapid growth of Internet-based applications, such as social media platforms and blogs, has resulted in comments and reviews concerning day-to-day activities. Sentiment analysis is the process of gathering and analyzing people's opinions, thoughts, and impressions regarding various topics, products, subjects, and services. People's opinions can be beneficial to corporations, governments, and individuals for collecting information and making decisions based on opinion. However, the sentiment analysis and evaluation procedure face numerous challenges. These challenges create impediments to accurately interpreting sentiments and determining the appropriate sentiment polarity. Sentiment analysis identifies and extracts subjective information from the text using natural language processing and text mining. This article discusses a complete overview of the method for completing this task as well as the applications of sentiment analysis. Then, it evaluates, compares, and investigates the approaches used to gain a comprehensive understanding of their advantages and disadvantages. Finally, the challenges of sentiment analysis are examined in order to define future directions.},
  langid = {english},
  keywords = {Machine learning,Sentiment analysis,Social media,Text analysis,Word embedding},
  file = {C:\Users\griswold\Zotero\storage\NRFJ38NZ\Wankhade et al. - 2022 - A survey on sentiment analysis methods, applicatio.pdf}
}

@misc{stewartLabelFreeSupervisionNeural2016,
  title = {Label-{{Free Supervision}} of {{Neural Networks}} with {{Physics}} and {{Domain Knowledge}}},
  author = {Stewart, Russell and Ermon, Stefano},
  year = {2016},
  month = sep,
  number = {arXiv:1609.05566},
  eprint = {1609.05566},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-11},
  abstract = {In many machine learning applications, labeled data is scarce and obtaining more labels is expensive. We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs. These constraints are derived from prior domain knowledge, e.g., from known laws of physics. We demonstrate the effectiveness of this approach on real world and simulated computer vision tasks. We are able to train a convolutional neural network to detect and track objects without any labeled examples. Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C:\Users\griswold\Zotero\storage\VYKS5C8C\Stewart and Ermon - 2016 - Label-Free Supervision of Neural Networks with Phy.pdf}
}


@article{roeFeatureEngineeringClinical2020,
  title = {Feature Engineering with Clinical Expert Knowledge: {{A}} Case Study Assessment of Machine Learning Model Complexity and Performance},
  shorttitle = {Feature Engineering with Clinical Expert Knowledge},
  author = {Roe, Kenneth D. and Jawa, Vibhu and Zhang, Xiaohan and Chute, Christopher G. and Epstein, Jeremy A. and Matelsky, Jordan and Shpitser, Ilya and Taylor, Casey Overby},
  year = {2020},
  month = apr,
  journal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0231300},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0231300},
  urldate = {2024-06-11},
  abstract = {Incorporating expert knowledge at the time machine learning models are trained holds promise for producing models that are easier to interpret. The main objectives of this study were to use a feature engineering approach to incorporate clinical expert knowledge prior to applying machine learning techniques, and to assess the impact of the approach on model complexity and performance. Four machine learning models were trained to predict mortality with a severe asthma case study. Experiments to select fewer input features based on a discriminative score showed low to moderate precision for discovering clinically meaningful triplets, indicating that discriminative score alone cannot replace clinical input. When compared to baseline machine learning models, we found a decrease in model complexity with use of fewer features informed by discriminative score and filtering of laboratory features with clinical input. We also found a small difference in performance for the mortality prediction task when comparing baseline ML models to models that used filtered features. Encoding demographic and triplet information in ML models with filtered features appeared to show performance improvements from the baseline. These findings indicated that the use of filtered features may reduce model complexity, and with little impact on performance.},
  langid = {english},
  keywords = {Asthma,Clinical laboratories,Electronic medical records,Laboratory tests,Machine learning,Machine learning algorithms,Medical risk factors,Neural networks},
  file = {C:\Users\griswold\Zotero\storage\IZLRYN7K\Roe et al. - 2020 - Feature engineering with clinical expert knowledge.pdf}
}


@article{vonruedenInformedMachineLearning2021,
  title = {Informed {{Machine Learning}} - {{A Taxonomy}} and {{Survey}} of {{Integrating Prior Knowledge}} into {{Learning Systems}}},
  author = {Von Rueden, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Walczak, Michal and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis},
  year = {2021},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2021.3079836},
  urldate = {2024-06-11},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  file = {C:\Users\griswold\Zotero\storage\RIKQEB4V\Von Rueden et al. - 2021 - Informed Machine Learning - A Taxonomy and Survey .pdf}
}


@article{widmannCreatingComparingDictionary2022,
  title = {Creating and {{Comparing Dictionary}}, {{Word Embedding}}, and {{Transformer-Based Models}} to {{Measure Discrete Emotions}} in {{German Political Text}}},
  author = {Widmann, Tobias and Wich, Maximilian},
  year = {2022},
  month = jun,
  journal = {Political Analysis},
  pages = {1--16},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2022.15},
  urldate = {2023-09-05},
  abstract = {Previous research on emotional language relied heavily on off-the-shelf sentiment dictionaries that focus on negative and positive tone. These dictionaries are often tailored to nonpolitical domains and use bag-of-words approaches which come with a series of disadvantages. This paper creates, validates, and compares the performance of (1) a novel emotional dictionary specifically for political text, (2) locally trained word embedding models combined with simple neural network classifiers, and (3) transformer-based models which overcome limitations of the dictionary approach. All tools can measure emotional appeals associated with eight discrete emotions. The different approaches are validated on different sets of crowd-coded sentences. Encouragingly, the results highlight the strengths of novel transformer-based models, which come with easily available pretrained language models. Furthermore, all customized approaches outperform widely used off-the-shelf dictionaries in measuring emotional language in German political discourse.},
  langid = {english},
  keywords = {dictionary,emotions,political text,text-as-data,transformer models,word embeddings},
  file = {C:\Users\griswold\Zotero\storage\TW2DRWDW\Widmann and Wich - 2022 - Creating and Comparing Dictionary, Word Embedding,.pdf}
}

@inproceedings{diligentiIntegratingPriorKnowledge2017,
  title = {Integrating {{Prior Knowledge}} into {{Deep Learning}}},
  booktitle = {2017 16th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Diligenti, Michelangelo and Roychowdhury, Soumali and Gori, Marco},
  year = {2017},
  month = dec,
  pages = {920--923},
  publisher = {IEEE},
  address = {Cancun, Mexico},
  doi = {10.1109/ICMLA.2017.00-37},
  urldate = {2024-06-11},
  isbn = {978-1-5386-1418-1}
}


@article{yarchiPoliticalPolarizationDigital2021,
  title = {Political {{Polarization}} on the {{Digital Sphere}}: {{A Cross-platform}}, {{Over-time Analysis}} of {{Interactional}}, {{Positional}}, and {{Affective Polarization}} on {{Social Media}}},
  shorttitle = {Political {{Polarization}} on the {{Digital Sphere}}},
  author = {Yarchi, Moran and Baden, Christian and {Kligler-Vilenchik}, Neta},
  year = {2021},
  month = mar,
  journal = {Political Communication},
  volume = {38},
  number = {1-2},
  pages = {98--139},
  publisher = {Routledge},
  issn = {1058-4609},
  doi = {10.1080/10584609.2020.1785067},
  urldate = {2024-01-02},
  abstract = {Political polarization on the digital sphere poses a real challenge to many democracies around the world. Although the issue has received some scholarly attention, there is a need to improve the conceptual precision in the increasingly blurry debate. The use of computational communication science approaches allows us to track political conversations in a fine-grained manner within their natural settings -- the realm of interactive social media. The present study combines different algorithmic approaches to studying social media data in order to capture both the interactional structure and content of dynamic political talk online. We conducted an analysis of political polarization across social media platforms (analyzing Facebook, Twitter, and WhatsApp) over 16~months, with close to a quarter million online contributions regarding a political controversy in Israel. Our comprehensive measurement of interactive political talk enables us to address three key aspects of political polarization: (1) interactional polarization -- homophilic versus heterophilic user interactions; (2) positional polarization -- the positions expressed, and (3) affective polarization -- the emotions and attitudes expressed. Our findings indicate that political polarization on social media cannot be conceptualized as a unified phenomenon, as there are significant cross-platform differences. While interactions on Twitter largely conform to established expectations (homophilic interaction patterns, aggravating positional polarization, pronounced inter-group hostility), on WhatsApp, de-polarization occurred over time. Surprisingly, Facebook was found to be the least homophilic platform in terms of interactions, positions, and emotions expressed. Our analysis points to key conceptual distinctions and raises important questions about the drivers and dynamics of political polarization online.},
  keywords = {computational communication science approach,cross-platform analysis,over-time analysis,Political polarization,social media},
  file = {C:\Users\griswold\Zotero\storage\77PRYUNZ\Yarchi et al. - 2021 - Political Polarization on the Digital Sphere A Cr.pdf}
}

@misc{zhangSentimentAnalysisEra2023,
  title = {Sentiment {{Analysis}} in the {{Era}} of {{Large Language Models}}: {{A Reality Check}}},
  shorttitle = {Sentiment {{Analysis}} in the {{Era}} of {{Large Language Models}}},
  author = {Zhang, Wenxuan and Deng, Yue and Liu, Bing and Pan, Sinno Jialin and Bing, Lidong},
  year = {2023},
  month = may,
  number = {arXiv:2305.15005},
  eprint = {2305.15005},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-13},
  abstract = {Sentiment analysis (SA) has been a longstanding research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, SENTIEVAL, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at https://github. com/DAMO-NLP-SG/LLM-Sentiment.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\griswold\Zotero\storage\U4AE3SWC\Zhang et al. - 2023 - Sentiment Analysis in the Era of Large Language Mo.pdf}
}

@misc{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-11},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\griswold\Zotero\storage\82PML6JR\Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf}
}

@misc{zhongCanChatGPTUnderstand2023,
  title = {Can {{ChatGPT Understand Too}}? {{A Comparative Study}} on {{ChatGPT}} and {{Fine-tuned BERT}}},
  shorttitle = {Can {{ChatGPT Understand Too}}?},
  author = {Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
  year = {2023},
  month = mar,
  number = {arXiv:2302.10198},
  eprint = {2302.10198},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-11},
  abstract = {Recently, ChatGPT has attracted great attention, as it can generate fluent and high-quality responses to human inquiries. Several prior studies have shown that ChatGPT attains remarkable generation ability compared with existing models. However, the quantitative analysis of ChatGPT's understanding ability has been given little attention. In this report, we explore the understanding ability of ChatGPT by evaluating it on the most popular GLUE benchmark, and comparing it with 4 representative fine-tuned BERT-style models. We find that: 1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT outperforms all BERT models on inference tasks by a large margin; 3) ChatGPT achieves comparable performance compared with BERT on sentiment analysis and question-answering tasks. Additionally, by combining some advanced prompting strategies, we show that the understanding ability of ChatGPT can be further improved.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\griswold\Zotero\storage\QIJT3P8D\Zhong et al. - 2023 - Can ChatGPT Understand Too A Comparative Study on.pdf}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{pollard24,
  title={A Demonstration of Propensity Score Weighting to Successfully Adjust a Social Media Convenience Sample Survey of Political Attitudes},
  author={Pollard, Michael S. and Robbins, Michael W. and Griswold, Max G.},
  journal={Preprint},
  year={2024}
}